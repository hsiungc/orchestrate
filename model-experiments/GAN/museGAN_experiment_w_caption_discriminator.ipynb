{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mido in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: packaging~=23.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from mido) (23.1)\n",
      "Requirement already satisfied: pygame in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: music21 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (7.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (3.5.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (5.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.3.2)\n",
      "Requirement already satisfied: webcolors>=1.5 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.12)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (8.13.0)\n",
      "Requirement already satisfied: jsonpickle in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ktrin\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mido\n",
    "!pip install pygame\n",
    "!pip install music21\n",
    "!pip install scikit-learn==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some useful libraries\n",
    "import glob, nltk, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from music21 import midi\n",
    "from plugins.midi2img import midi2img\n",
    "from plugins.img2midi import img2midi\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load In museGAN dataset for visualization purposes\n",
    "It turned out that the people at museGAN is leveraging midi -> image conversion. The image consisted of bar of a multi track piano roll. From the below image, the horizontal represent time and the vericle represent the instrument used. In this dataset the instrument are layered from bottom to top as piano, strings, guitar, drums, bass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ktrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the punkt tokenizer from nltk to tokenize the piece caption\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of museGAN\n",
    "## External Data Source\n",
    "For whatever reason, if we wanted to perform GAN modeling, we can leverage conversion of MIDI data to that of the piano roll. Download the data from piano repo in README and start performing the things below. [Convert-MIDI-TO-NP-ARRAY](https://medium.com/analytics-vidhya/convert-midi-file-to-numpy-array-in-python-7d00531890c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 20 # 18 + start, end\n",
    "EMBED_DIM = 100 \n",
    "MAX_VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>piece_id</th>\n",
       "      <th>piece_description</th>\n",
       "      <th>piece_arousal</th>\n",
       "      <th>piece_name</th>\n",
       "      <th>midi_file</th>\n",
       "      <th>caption_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>very upbeat</td>\n",
       "      <td>Delighted</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>very upbeat. delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I could tell the valence of the example was in...</td>\n",
       "      <td>Valence started out moderately negative and pr...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>i could tell the valence of the example was in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>For a second I thought this piece was going to...</td>\n",
       "      <td>This piece seemed to have a positive valence t...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>for a second i thought this piece was going to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bouncy and fun</td>\n",
       "      <td>Kind of sparatic</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>bouncy and fun. kind of sparatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>nice. nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>6100</td>\n",
       "      <td>It started off slowly but happy and then built...</td>\n",
       "      <td>Seemed to remain consistent almost like it was...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>it started off slowly but happy and then built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>6101</td>\n",
       "      <td>This starts off a certain way then changes in ...</td>\n",
       "      <td>This is nostalgic because I recognize this and...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>this starts off a certain way then changes in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6102</td>\n",
       "      <td>The piece begins slow in tempo and then become...</td>\n",
       "      <td>The beginning rhythm sounds suspenseful, makin...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the piece begins slow in tempo and then become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6103</td>\n",
       "      <td>started slow but picked up.</td>\n",
       "      <td>I feel it stayed the same</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>started slow but picked up. . i feel it stayed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>6104</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>this music sounds intense. it doesn't change m...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6105 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      piece_id                                  piece_description  \\\n",
       "0            0                                        very upbeat   \n",
       "1            1  I could tell the valence of the example was in...   \n",
       "2            2  For a second I thought this piece was going to...   \n",
       "3            3                                     Bouncy and fun   \n",
       "4            4                                               nice   \n",
       "...        ...                                                ...   \n",
       "6100      6100  It started off slowly but happy and then built...   \n",
       "6101      6101  This starts off a certain way then changes in ...   \n",
       "6102      6102  The piece begins slow in tempo and then become...   \n",
       "6103      6103                       started slow but picked up.    \n",
       "6104      6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                          piece_arousal  \\\n",
       "0                                             Delighted   \n",
       "1     Valence started out moderately negative and pr...   \n",
       "2     This piece seemed to have a positive valence t...   \n",
       "3                                      Kind of sparatic   \n",
       "4                                                  nice   \n",
       "...                                                 ...   \n",
       "6100  Seemed to remain consistent almost like it was...   \n",
       "6101  This is nostalgic because I recognize this and...   \n",
       "6102  The beginning rhythm sounds suspenseful, makin...   \n",
       "6103                         I feel it stayed the same    \n",
       "6104  this music sounds intense. it doesn't change m...   \n",
       "\n",
       "                   piece_name  \\\n",
       "0     Lurking In The Darkness   \n",
       "1     Lurking In The Darkness   \n",
       "2     Lurking In The Darkness   \n",
       "3     Lurking In The Darkness   \n",
       "4     Lurking In The Darkness   \n",
       "...                       ...   \n",
       "6100         One Winged Angel   \n",
       "6101         One Winged Angel   \n",
       "6102         One Winged Angel   \n",
       "6103         One Winged Angel   \n",
       "6104         One Winged Angel   \n",
       "\n",
       "                                              midi_file  \\\n",
       "0     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "1     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "2     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "3     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "4     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "...                                                 ...   \n",
       "6100  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6101  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6102  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6103  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6104  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "\n",
       "                                           caption_list  \n",
       "0                                very upbeat. delighted  \n",
       "1     i could tell the valence of the example was in...  \n",
       "2     for a second i thought this piece was going to...  \n",
       "3                      bouncy and fun. kind of sparatic  \n",
       "4                                            nice. nice  \n",
       "...                                                 ...  \n",
       "6100  it started off slowly but happy and then built...  \n",
       "6101  this starts off a certain way then changes in ...  \n",
       "6102  the piece begins slow in tempo and then become...  \n",
       "6103  started slow but picked up. . i feel it stayed...  \n",
       "6104  the music is kind of happy and then starts slo...  \n",
       "\n",
       "[6105 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the metadata\n",
    "# create a list of captions that concatenate the piece description and arousal\n",
    "# lower case te caption list\n",
    "midi_meta = pd.read_csv('../data/piano-labelled/labelled_piano_midi_metadata.csv')\n",
    "midi_meta['caption_list'] = midi_meta['piece_description'].str.lower()+ \". \" + midi_meta['piece_arousal'].str.lower()\n",
    "midi_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dd8a221dd34c7fac2c00cdc18d5308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a vocabulary using sklearn count vectorizer to create a vocab from the most frequent words\n",
    "input_captions = []\n",
    "max_caption_length = -1 \n",
    "\n",
    "for caption in tqdm(midi_meta['caption_list'].values):\n",
    "    tokenized_caption = nltk.word_tokenize(caption, language='english')\n",
    "\n",
    "    if len(tokenized_caption) > max_caption_length:\n",
    "        max_caption_length = len(tokenized_caption)\n",
    "\n",
    "    caption = (' '.join(tokenized_caption)).lower()\n",
    "    input_captions.append(caption)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=MAX_VOCAB_SIZE)\n",
    "vectorizer.fit(input_captions)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "MAX_VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn vocab into a dictionary of words and token id\n",
    "# replace some words with special tokens like start/end/unk\n",
    "# if the caption is too short, pad it with <pad> token\n",
    "id_vocab_dict = {}\n",
    "vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab):\n",
    "    id_vocab_dict[sid] = svocab\n",
    "    vocab_id_dict[svocab] = sid\n",
    "\n",
    "id_vocab_dict[MAX_VOCAB_SIZE] = \"<unk>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 1] = \"<start>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 2] = \"<end>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 3] = \"<pad>\"\n",
    "\n",
    "vocab_id_dict[\"<unk>\"] = MAX_VOCAB_SIZE\n",
    "vocab_id_dict[\"<start>\"] = MAX_VOCAB_SIZE + 1\n",
    "vocab_id_dict[\"<end>\"] = MAX_VOCAB_SIZE + 2\n",
    "vocab_id_dict[\"<pad>\"] = MAX_VOCAB_SIZE + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization - take the input caption and tokenize it\n",
    "# declare a max sequence length \n",
    "def convert_text_to_data(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None):\n",
    "    \"\"\"\n",
    "        Function to convert text based data into tokenized data with proper padding\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = []\n",
    "    for text_num, text in enumerate(texts):\n",
    "        sentence_ids = []\n",
    "\n",
    "        # split the sentence into token\n",
    "        # use the vocab to turn the word token into number\n",
    "        for token in text.split():\n",
    "            if token in vocab_id_dict.keys():\n",
    "                sentence_ids.append(vocab_id_dict[token])\n",
    "            else:\n",
    "                sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "\n",
    "        vocab_size = len(vocab_id_dict.keys())\n",
    "\n",
    "        # for decoder cases:\n",
    "        # input sentence: <start>, [tokenize words from vocab], <end>, padded with <unk>\n",
    "        # ouput sentence has: [tokenize words from vocab], <end>, padded with <unk>\n",
    "        if type == 'input_target':\n",
    "            ids = ([vocab_size - 3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        elif type == 'output_target':\n",
    "            ids = (sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        processed_data.append(ids)\n",
    "\n",
    "    return np.array(processed_data)\n",
    "\n",
    "\n",
    "train_target_input_data = convert_text_to_data(input_captions,\n",
    "                                                vocab_id_dict,\n",
    "                                                type='input_target',\n",
    "                                                max_length=MAX_SEQ_LENGTH)\n",
    "len(train_target_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>piece_id</th>\n",
       "      <th>piece_description</th>\n",
       "      <th>piece_arousal</th>\n",
       "      <th>piece_name</th>\n",
       "      <th>midi_file</th>\n",
       "      <th>caption_list</th>\n",
       "      <th>tokenized_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>very upbeat</td>\n",
       "      <td>Delighted</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>very upbeat. delighted</td>\n",
       "      <td>[3452, 3319, 3258, 3451, 762, 3453, 3454, 3454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I could tell the valence of the example was in...</td>\n",
       "      <td>Valence started out moderately negative and pr...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>i could tell the valence of the example was in...</td>\n",
       "      <td>[3452, 3451, 661, 3025, 3053, 3290, 2095, 3053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>For a second I thought this piece was going to...</td>\n",
       "      <td>This piece seemed to have a positive valence t...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>for a second i thought this piece was going to...</td>\n",
       "      <td>[3452, 1220, 3451, 2627, 3451, 3078, 3074, 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bouncy and fun</td>\n",
       "      <td>Kind of sparatic</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>bouncy and fun. kind of sparatic</td>\n",
       "      <td>[3452, 364, 138, 1262, 3451, 1686, 2095, 2820,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>nice. nice</td>\n",
       "      <td>[3452, 2044, 3451, 2044, 3453, 3454, 3454, 345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>6100</td>\n",
       "      <td>It started off slowly but happy and then built...</td>\n",
       "      <td>Seemed to remain consistent almost like it was...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>it started off slowly but happy and then built...</td>\n",
       "      <td>[3452, 1623, 2869, 2096, 2756, 413, 1374, 138,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>6101</td>\n",
       "      <td>This starts off a certain way then changes in ...</td>\n",
       "      <td>This is nostalgic because I recognize this and...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>this starts off a certain way then changes in ...</td>\n",
       "      <td>[3452, 3074, 2872, 2096, 3451, 468, 3366, 3061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6102</td>\n",
       "      <td>The piece begins slow in tempo and then become...</td>\n",
       "      <td>The beginning rhythm sounds suspenseful, makin...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the piece begins slow in tempo and then become...</td>\n",
       "      <td>[3452, 3053, 2241, 304, 2751, 1510, 3031, 138,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6103</td>\n",
       "      <td>started slow but picked up.</td>\n",
       "      <td>I feel it stayed the same</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>started slow but picked up. . i feel it stayed...</td>\n",
       "      <td>[3452, 2869, 2751, 413, 2233, 3451, 3451, 3451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>6104</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>this music sounds intense. it doesn't change m...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>[3452, 3053, 1996, 1618, 1686, 2095, 1374, 138...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      piece_id                                  piece_description  \\\n",
       "0            0                                        very upbeat   \n",
       "1            1  I could tell the valence of the example was in...   \n",
       "2            2  For a second I thought this piece was going to...   \n",
       "3            3                                     Bouncy and fun   \n",
       "4            4                                               nice   \n",
       "...        ...                                                ...   \n",
       "6100      6100  It started off slowly but happy and then built...   \n",
       "6101      6101  This starts off a certain way then changes in ...   \n",
       "6102      6102  The piece begins slow in tempo and then become...   \n",
       "6103      6103                       started slow but picked up.    \n",
       "6104      6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                          piece_arousal  \\\n",
       "0                                             Delighted   \n",
       "1     Valence started out moderately negative and pr...   \n",
       "2     This piece seemed to have a positive valence t...   \n",
       "3                                      Kind of sparatic   \n",
       "4                                                  nice   \n",
       "...                                                 ...   \n",
       "6100  Seemed to remain consistent almost like it was...   \n",
       "6101  This is nostalgic because I recognize this and...   \n",
       "6102  The beginning rhythm sounds suspenseful, makin...   \n",
       "6103                         I feel it stayed the same    \n",
       "6104  this music sounds intense. it doesn't change m...   \n",
       "\n",
       "                   piece_name  \\\n",
       "0     Lurking In The Darkness   \n",
       "1     Lurking In The Darkness   \n",
       "2     Lurking In The Darkness   \n",
       "3     Lurking In The Darkness   \n",
       "4     Lurking In The Darkness   \n",
       "...                       ...   \n",
       "6100         One Winged Angel   \n",
       "6101         One Winged Angel   \n",
       "6102         One Winged Angel   \n",
       "6103         One Winged Angel   \n",
       "6104         One Winged Angel   \n",
       "\n",
       "                                              midi_file  \\\n",
       "0     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "1     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "2     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "3     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "4     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "...                                                 ...   \n",
       "6100  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6101  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6102  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6103  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6104  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "\n",
       "                                           caption_list  \\\n",
       "0                                very upbeat. delighted   \n",
       "1     i could tell the valence of the example was in...   \n",
       "2     for a second i thought this piece was going to...   \n",
       "3                      bouncy and fun. kind of sparatic   \n",
       "4                                            nice. nice   \n",
       "...                                                 ...   \n",
       "6100  it started off slowly but happy and then built...   \n",
       "6101  this starts off a certain way then changes in ...   \n",
       "6102  the piece begins slow in tempo and then become...   \n",
       "6103  started slow but picked up. . i feel it stayed...   \n",
       "6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                     tokenized_captions  \n",
       "0     [3452, 3319, 3258, 3451, 762, 3453, 3454, 3454...  \n",
       "1     [3452, 3451, 661, 3025, 3053, 3290, 2095, 3053...  \n",
       "2     [3452, 1220, 3451, 2627, 3451, 3078, 3074, 224...  \n",
       "3     [3452, 364, 138, 1262, 3451, 1686, 2095, 2820,...  \n",
       "4     [3452, 2044, 3451, 2044, 3453, 3454, 3454, 345...  \n",
       "...                                                 ...  \n",
       "6100  [3452, 1623, 2869, 2096, 2756, 413, 1374, 138,...  \n",
       "6101  [3452, 3074, 2872, 2096, 3451, 468, 3366, 3061...  \n",
       "6102  [3452, 3053, 2241, 304, 2751, 1510, 3031, 138,...  \n",
       "6103  [3452, 2869, 2751, 413, 2233, 3451, 3451, 3451...  \n",
       "6104  [3452, 3053, 1996, 1618, 1686, 2095, 1374, 138...  \n",
       "\n",
       "[6105 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# added the tokenized caption to the metadata\n",
    "midi_meta['tokenized_captions'] = train_target_input_data.tolist()\n",
    "midi_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Definition\n",
    "\n",
    "GAN model consists of two part:\n",
    "1. Generator\n",
    "2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# check to see if tensorflow mount to GPU properly\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_generator(latent_dim=100, \n",
    "                               caption_dim=MAX_SEQ_LENGTH, \n",
    "                               vocab_size=len(vocab_id_dict.keys()), \n",
    "                               embed_dim=EMBED_DIM):\n",
    "    \"\"\"Define the generator model\n",
    "        Inputs:\n",
    "            latent_dim: dimension of the latent space\n",
    "        Output:\n",
    "            model: the generator model\n",
    "    \"\"\"\n",
    "    n_nodes = 128 * 53 * 53\n",
    "\n",
    "    # vectorized input layers\n",
    "    input_layer = keras.layers.Input(shape=(latent_dim,), name='input_layer')\n",
    "    \n",
    "    # # vectorized caption input layers\n",
    "    # # apply word embedding to the caption\n",
    "    caption_input_layer = keras.layers.Input(shape=(caption_dim,), name='caption_input_layer')\n",
    "    embedding_layer  = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=embed_dim,\n",
    "                                                name='caption_embedding_layer')\n",
    "    embed_caption = embedding_layer(caption_input_layer)\n",
    "\n",
    "    # # source_image_encoding = keras.layers.GlobalAveragePooling2D()(dense4)\n",
    "    # # using LSTM to encode the caption with the input layer\n",
    "    lstm_layer = keras.layers.LSTM(100, return_sequences=True, return_state=True, name=\"decoder_lstm_layer\")\n",
    "    decoder_output, decoder_state_h_output, decoder_state_c_output = lstm_layer(embed_caption, initial_state=[input_layer, input_layer])\n",
    "\n",
    "    # apply 1D Global Average Pooling to the output of the dense layer on the caption decoded\n",
    "    # global_average_pooling1d_layer = keras.layers.GlobalAveragePooling1D()(decoder_output)\n",
    "\n",
    "    # Dense Layer 1\n",
    "    dense1 = keras.layers.Dense(n_nodes, activation='relu')(decoder_state_c_output)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.2)(dense1)\n",
    "    reshape_layer = keras.layers.Reshape((53, 53, 128))(leaky_relu1)\n",
    "\n",
    "    # Dense Layer 2\n",
    "    dense2 =  keras.layers.Dense(1024)(reshape_layer)\n",
    "\n",
    "    # Conv2DTranspose Layer\n",
    "    conv2d_transpose = keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same')(dense2)\n",
    "\n",
    "    # Dense Layer 3\n",
    "    dense3 =  keras.layers.Dense(1024)(conv2d_transpose)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.2)(dense3)\n",
    "\n",
    "    # Dense Layer 4\n",
    "    dense4 =  keras.layers.Dense(1024)(leaky_relu2)\n",
    "\n",
    "    # Conv2D Layer\n",
    "    conv2d = keras.layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')(dense4)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[input_layer,caption_input_layer], outputs=conv2d, name='generator_model')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_discriminator(in_shape = (106,106,1),\n",
    "                                   caption_dim=MAX_SEQ_LENGTH, \n",
    "                                   vocab_size=len(vocab_id_dict.keys()), \n",
    "                                   embed_dim=EMBED_DIM):\n",
    "    \"\"\"\n",
    "        GAN discriminator model\n",
    "        Inputs:\n",
    "            in_shape: shape of the input image\n",
    "        Output:\n",
    "            model: discriminator model with binary crossentropy loss to denotes if the image is real or fake\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = keras.layers.Input(shape=in_shape, name='input_layer')\n",
    "    \n",
    "\n",
    "    # 2D Convlution Layer 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(input_layer)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    dropout1 = keras.layers.Dropout(0.5)(leaky_relu1)\n",
    "\n",
    "    # 2D Convlution Layer 2\n",
    "    conv2 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(dropout1)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    dropout2 = keras.layers.Dropout(0.5)(leaky_relu2)\n",
    "\n",
    "    # Flatten Layer\n",
    "    flatten_layer = keras.layers.Flatten()(dropout2)\n",
    "\n",
    "\n",
    "    # Batch Normalization Layer\n",
    "    batch_normalization = keras.layers.BatchNormalization()(flatten_layer)\n",
    "\n",
    "    # Reduced Dense Layer\n",
    "    reduced_dense = keras.layers.Dense(100)(batch_normalization)\n",
    "\n",
    "\n",
    "    ## vectorized caption input layers\n",
    "    # # apply word embedding to the caption\n",
    "    caption_input_layer = keras.layers.Input(shape=(caption_dim,), name='caption_input_layer')\n",
    "    embedding_layer  = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=embed_dim,\n",
    "                                                name='caption_embedding_layer')\n",
    "    embed_caption = embedding_layer(caption_input_layer)\n",
    "\n",
    "    # # source_image_encoding = keras.layers.GlobalAveragePooling2D()(dense4)\n",
    "    # # using LSTM to encode the caption with the input layer\n",
    "    lstm_layer = keras.layers.LSTM(100, return_sequences=True, return_state=True, name=\"decoder_lstm_layer\")\n",
    "    decoder_output, decoder_state_h_output, decoder_state_c_output = lstm_layer(embed_caption, initial_state=[reduced_dense, reduced_dense])\n",
    "\n",
    "    dense_layer = keras.layers.Dense(1024)(batch_normalization)\n",
    "\n",
    "    # Dense Output Disminator Layer\n",
    "    discriminate_layer = keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n",
    "    \n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[input_layer, caption_input_layer], outputs=discriminate_layer, name='discriminator_model')\n",
    "    \n",
    "    # # model compile\n",
    "    # opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.d_gradient = []\n",
    "        self.g_gradient = []\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        # Unpack the data\n",
    "        real_images, real_captions = dataset\n",
    "\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator([fake_images, real_captions], training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator([real_images, real_captions], training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_loss = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_gradient.append((i, d_gradient))\n",
    "\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator([generated_images, real_captions], training=True)\n",
    "\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        self.g_gradient.append(gen_gradient)\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # slice the image and caption from the dataset\n",
    "    # generate 'real' class labels (1)\n",
    "    images, captions = zip(*dataset)\n",
    "    images = np.array(images)\n",
    "    captions = np.array(captions)\n",
    "    ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    X_img = images[ix]   \n",
    "    X_cap = captions[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X_img, X_cap, y\n",
    " \n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(g_model, latent_dim, caption, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = g_model.predict([x_input, caption])\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input_layer (InputLayer [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embedding_layer (Embedd (None, 20, 100)      345500      caption_input_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_layer (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_layer (LSTM)       [(None, 20, 100), (N 80400       caption_embedding_layer[0][0]    \n",
      "                                                                 input_layer[0][0]                \n",
      "                                                                 input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 359552)       36314752    decoder_lstm_layer[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 359552)       0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 53, 53, 128)  0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 53, 53, 1024) 132096      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 106, 106, 102 16778240    dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 106, 106, 102 1049600     conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 106, 106, 102 0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 106, 106, 102 1049600     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 106, 106, 1)  50177       dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 55,800,365\n",
      "Trainable params: 55,800,365\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 106, 106, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 53, 53, 64)   640         input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 53, 53, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 53, 53, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 27, 27, 64)   36928       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 27, 27, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 27, 27, 64)   0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 46656)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 46656)        186624      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         47776768    batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "caption_input_layer (InputLayer [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            1025        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 48,001,985\n",
      "Trainable params: 47,908,673\n",
      "Non-trainable params: 93,312\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "g_model = caption_enhanced_generator(latent_dim)\n",
    "d_model = caption_enhanced_discriminator()\n",
    "g_model.summary(), d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the generated model\n",
    "keras.utils.plot_model(g_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(d_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(gan_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "import random\n",
    "random.seed(5634)\n",
    "\n",
    "training_set = joblib.load('../data/piano-labelled/training_set.joblib')\n",
    "training_set = random.sample(training_set, 2048) #5 is the lenth of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (2048, 106, 106, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unzip the dataset\n",
    "images, captions = zip(*training_set)\n",
    "images = np.array(images)\n",
    "captions = np.array(captions)\n",
    "type(images), images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museGAN_experiment_w_caption_discriminator.ipynb Cell 25\u001b[0m line \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m gan\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     d_optimizer\u001b[39m=\u001b[39mdiscriminator_optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     g_optimizer\u001b[39m=\u001b[39mgenerator_optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     g_loss_fn\u001b[39m=\u001b[39mgenerator_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     d_loss_fn\u001b[39m=\u001b[39mdiscriminator_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m gan\u001b[39m.\u001b[39;49mfit(images, captions, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m    934\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:759\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph \u001b[39m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    757\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_deleter \u001b[39m=\u001b[39m FunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    758\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_stateful_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 759\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    760\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    762\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    763\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:3066\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3065\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 3066\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   3067\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   3464\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[0;32m   3466\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   3299\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   3300\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   3301\u001b[0m         args,\n\u001b[0;32m   3302\u001b[0m         kwargs,\n\u001b[0;32m   3303\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   3304\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   3305\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   3306\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   3307\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[0;32m   3308\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   3309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1007\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1009\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1012\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:668\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    666\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 668\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39m__wrapped__(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    669\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py:983\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[39m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 983\u001b[0m   \u001b[39mreturn\u001b[39;00m autograph\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[0;32m    984\u001b[0m       original_func,\n\u001b[0;32m    985\u001b[0m       args,\n\u001b[0;32m    986\u001b[0m       kwargs,\n\u001b[0;32m    987\u001b[0m       options\u001b[39m=\u001b[39;49mautograph\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[0;32m    988\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    989\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[0;32m    990\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    991\u001b[0m       ))\n\u001b[0;32m    992\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    993\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:444\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    445\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\tmp4jwq7fbl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:382\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    379\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 382\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    384\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 464\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\keras\\engine\\training.py:842\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m    839\u001b[0m   \u001b[39mreturn\u001b[39;00m outputs\n\u001b[0;32m    841\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m--> 842\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m    843\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    844\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    845\u001b[0m write_scalar_summaries(outputs, step\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39m_train_counter)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1282\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   1285\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1286\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2847\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   2848\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 2849\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   3631\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> 3632\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    693\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    694\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:382\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    379\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 382\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    384\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:463\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    460\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    464\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\keras\\engine\\training.py:835\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m--> 835\u001b[0m   outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m    836\u001b[0m   \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m    837\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museGAN_experiment_w_caption_discriminator.ipynb Cell 25\u001b[0m line \u001b[0;36mGAN.train_step\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     d_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_loss_fn(real_img\u001b[39m=\u001b[39mreal_logits, fake_img\u001b[39m=\u001b[39mfake_logits)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Get the gradients w.r.t the discriminator loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m d_gradient \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(d_loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiscriminator\u001b[39m.\u001b[39;49mtrainable_variables)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_gradient\u001b[39m.\u001b[39mappend((i, d_gradient))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X46sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Update the weights of the discriminator using the discriminator optimizer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py:1084\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m output_gradients \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1081\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1082\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(output_gradients)]\n\u001b[1;32m-> 1084\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1085\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1086\u001b[0m     flat_targets,\n\u001b[0;32m   1087\u001b[0m     flat_sources,\n\u001b[0;32m   1088\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1089\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1090\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1093\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:71\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 71\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     72\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     73\u001b[0m     target,\n\u001b[0;32m     74\u001b[0m     sources,\n\u001b[0;32m     75\u001b[0m     output_gradients,\n\u001b[0;32m     76\u001b[0m     sources_raw,\n\u001b[0;32m     77\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py:159\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    157\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 159\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1664\u001b[0m, in \u001b[0;36m_SelectGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1662\u001b[0m c \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1663\u001b[0m x \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m-> 1664\u001b[0m zeros \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49mzeros_like(x)\n\u001b[0;32m   1665\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, array_ops\u001b[39m.\u001b[39mwhere(c, grad, zeros), array_ops\u001b[39m.\u001b[39mwhere(\n\u001b[0;32m   1666\u001b[0m     c, zeros, grad))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3017\u001b[0m, in \u001b[0;36mzeros_like\u001b[1;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[0;32m   2981\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mzeros_like\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   2982\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   2983\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mzeros_like\u001b[39m(tensor, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, optimize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   2984\u001b[0m   \u001b[39m\"\"\"Creates a tensor with all elements set to zero.\u001b[39;00m\n\u001b[0;32m   2985\u001b[0m \n\u001b[0;32m   2986\u001b[0m \u001b[39m  See also `tf.zeros`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3015\u001b[0m \u001b[39m    A `Tensor` with all elements set to zero.\u001b[39;00m\n\u001b[0;32m   3016\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3017\u001b[0m   \u001b[39mreturn\u001b[39;00m zeros_like_impl(tensor, dtype, name, optimize)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2915\u001b[0m, in \u001b[0;36m_tag_zeros_tensor.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2915\u001b[0m   tensor \u001b[39m=\u001b[39m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2916\u001b[0m   tensor\u001b[39m.\u001b[39m_is_zeros_tensor \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2917\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3087\u001b[0m, in \u001b[0;36mzeros_like_impl\u001b[1;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[0;32m   3080\u001b[0m \u001b[39m# For now, variant types must be created via zeros_like; as we need to\u001b[39;00m\n\u001b[0;32m   3081\u001b[0m \u001b[39m# pass the input variant object to the proper zeros callback.\u001b[39;00m\n\u001b[0;32m   3083\u001b[0m \u001b[39mif\u001b[39;00m (optimize \u001b[39mand\u001b[39;00m tensor_shape\u001b[39m.\u001b[39mis_fully_defined() \u001b[39mand\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m     tensor_dtype \u001b[39m!=\u001b[39m dtypes\u001b[39m.\u001b[39mvariant):\n\u001b[0;32m   3085\u001b[0m   \u001b[39m# We can produce a zeros tensor independent of the value of 'tensor',\u001b[39;00m\n\u001b[0;32m   3086\u001b[0m   \u001b[39m# since the shape is known statically.\u001b[39;00m\n\u001b[1;32m-> 3087\u001b[0m   \u001b[39mreturn\u001b[39;00m zeros(tensor_shape, dtype\u001b[39m=\u001b[39;49mdtype \u001b[39mor\u001b[39;49;00m tensor_dtype, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   3089\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dtype \u001b[39m!=\u001b[39m tensor_dtype \u001b[39mand\u001b[39;00m dtype \u001b[39m!=\u001b[39m dtypes\u001b[39m.\u001b[39mvariant:\n\u001b[0;32m   3090\u001b[0m   \u001b[39mreturn\u001b[39;00m zeros(\n\u001b[0;32m   3091\u001b[0m       shape_internal(tensor, optimize\u001b[39m=\u001b[39moptimize), dtype\u001b[39m=\u001b[39mdtype, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[39m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2915\u001b[0m, in \u001b[0;36m_tag_zeros_tensor.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2915\u001b[0m   tensor \u001b[39m=\u001b[39m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2916\u001b[0m   tensor\u001b[39m.\u001b[39m_is_zeros_tensor \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2917\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2964\u001b[0m, in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   2960\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2961\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m   2962\u001b[0m     \u001b[39m# Create a constant if it won't be very big. Otherwise create a fill\u001b[39;00m\n\u001b[0;32m   2963\u001b[0m     \u001b[39m# op to prevent serialized GraphDefs from becoming too large.\u001b[39;00m\n\u001b[1;32m-> 2964\u001b[0m     output \u001b[39m=\u001b[39m _constant_if_small(zero, shape, dtype, name)\n\u001b[0;32m   2965\u001b[0m     \u001b[39mif\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2966\u001b[0m       \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2901\u001b[0m, in \u001b[0;36m_constant_if_small\u001b[1;34m(value, shape, dtype, name)\u001b[0m\n\u001b[0;32m   2899\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2900\u001b[0m   \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mprod(shape) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[1;32m-> 2901\u001b[0m     \u001b[39mreturn\u001b[39;00m constant(value, shape\u001b[39m=\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   2902\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mNotImplementedError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m   2903\u001b[0m   \u001b[39m# Happens when shape is a Tensor, list with Tensor elements, etc.\u001b[39;00m\n\u001b[0;32m   2904\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    176\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    272\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py:293\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    291\u001b[0m dtype_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    292\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: tensor_value, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 293\u001b[0m const_tensor \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    294\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mConst\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype_value\u001b[39m.\u001b[39;49mtype], attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m op_callbacks\u001b[39m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    297\u001b[0m   \u001b[39m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    298\u001b[0m   \u001b[39m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m   callback_outputs \u001b[39m=\u001b[39m op_callbacks\u001b[39m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    300\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[39m=\u001b[39mname, graph\u001b[39m=\u001b[39mg)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    597\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    598\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    600\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    601\u001b[0m     compute_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:3561\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3558\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3560\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3561\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[0;32m   3562\u001b[0m       node_def,\n\u001b[0;32m   3563\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3564\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3565\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3566\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3567\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3568\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3569\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   3570\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3571\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:2041\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2039\u001b[0m   \u001b[39mif\u001b[39;00m op_def \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2040\u001b[0m     op_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39m_get_op_def(node_def\u001b[39m.\u001b[39mop)\n\u001b[1;32m-> 2041\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op \u001b[39m=\u001b[39m _create_c_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, node_def, inputs,\n\u001b[0;32m   2042\u001b[0m                             control_input_ops, op_def)\n\u001b[0;32m   2043\u001b[0m   name \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mname)\n\u001b[0;32m   2045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traceback \u001b[39m=\u001b[39m tf_stack\u001b[39m.\u001b[39mextract_stack_for_node(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py:1880\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1876\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[39m.\u001b[39mas_str(name),\n\u001b[0;32m   1877\u001b[0m                                          serialized)\n\u001b[0;32m   1879\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1880\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_FinishOperation(op_desc)\n\u001b[0;32m   1881\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1882\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1883\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mstr\u001b[39m(e))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set some training parameters\n",
    "BATCH_SIZE = 8\n",
    "noise_dim = 100\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9,\n",
    "    clipvalue=0.2\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9,\n",
    "    clipnorm=2.0\n",
    ")\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    # real_loss = tf.reduce_mean(real_img)\n",
    "    # fake_loss = tf.reduce_mean(fake_img)\n",
    "    real_loss = cross_entropy(tf.ones_like(real_img), real_img)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_img), fake_img) \n",
    "    return fake_loss + real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "    # return cross_entropy(tf.ones_like(fake_img), fake_img)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "# cbk = GANMonitor(num_img=3, latent_dim=noise_dim, filepath='../model/wgan-callbacks')\n",
    "\n",
    "# Get the wgan model\n",
    "gan = GAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "gan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "gan.fit(images, captions, batch_size=BATCH_SIZE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "g_model.save('../models/mini-gan/caption_piano_generator_model_agumented.h5')\n",
    "d_model.save('../models/mini-gan/caption_piano_discriminator_model_agumented.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# g_model = keras.models.load_model('../models/mini-gan/caption_piano_generator_model_agumented.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['it goes from forced to worried . determination'], dtype=object),\n",
       " array([[3452, 1623, 1307, 1253, 1223, 3122, 3423, 3451,  800, 3453, 3454,\n",
       "         3454, 3454, 3454, 3454, 3454, 3454, 3454, 3454, 3454]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random row from the metadata to get the caption\n",
    "row = midi_meta.sample(1, random_state=63)\n",
    "\n",
    "# get a random image tokenize caption and actual caption\n",
    "NLP_caption = row['caption_list'].values\n",
    "caption = [np.array(a) for a in row['tokenized_captions'].values]\n",
    "caption = np.array(caption)\n",
    "NLP_caption, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (1, 20))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "\n",
    "# declare a latent space\n",
    "latent_dim = 100\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "latent_points.shape, caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = g_model\n",
    "X = g_model.predict([latent_points, caption])\n",
    "array = np.array(X.reshape(106,106),dtype = np.uint8)\n",
    "np.unique(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "array*=255\n",
    "new_image = Image.fromarray(array,'L')\n",
    "new_image = new_image.save(f'../data/midi_reconstruction/images/captioned_piece_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconvert MIDI images to MIDI files\n",
    "image_path = \"../data/midi_reconstruction/images/captioned_piece_test.png\"\n",
    "output_path = \"../data/midi_reconstruction\"\n",
    "\n",
    "img2midi_obj = img2midi(image_path, output_path, resolution=0.25)\n",
    "img2midi_obj.convert_to_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museGAN_experiment_w_caption_discriminator.ipynb Cell 33\u001b[0m line \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m s \u001b[39m=\u001b[39m midi\u001b[39m.\u001b[39mtranslate\u001b[39m.\u001b[39mmidiFileToStream(mf)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m sp \u001b[39m=\u001b[39m midi\u001b[39m.\u001b[39mrealtime\u001b[39m.\u001b[39mStreamPlayer(s)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_discriminator.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m sp\u001b[39m.\u001b[39;49mplay()\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\music21\\midi\\realtime.py:124\u001b[0m, in \u001b[0;36mStreamPlayer.play\u001b[1;34m(self, busyFunction, busyArgs, endFunction, endArgs, busyWaitMilliseconds, playForMilliseconds, blocked)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mbusyFunction is a function that is called with busyArgs when the music is busy every\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39mbusyWaitMilliseconds.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39myou to completely control whether to stop it. Ignore every other arguments\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    123\u001b[0m streamStringIOFile \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetStringOrBytesIOFile()\n\u001b[1;32m--> 124\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplayStringIOFile(streamStringIOFile, busyFunction, busyArgs,\n\u001b[0;32m    125\u001b[0m                       endFunction, endArgs, busyWaitMilliseconds,\n\u001b[0;32m    126\u001b[0m                       playForMilliseconds\u001b[39m=\u001b[39;49mplayForMilliseconds, blocked\u001b[39m=\u001b[39;49mblocked)\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\music21\\midi\\realtime.py:166\u001b[0m, in \u001b[0;36mStreamPlayer.playStringIOFile\u001b[1;34m(self, stringIOFile, busyFunction, busyArgs, endFunction, endArgs, busyWaitMilliseconds, playForMilliseconds, blocked)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpygame\u001b[39m.\u001b[39mmixer\u001b[39m.\u001b[39mmusic\u001b[39m.\u001b[39mstop()\n\u001b[0;32m    165\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     pygameClock\u001b[39m.\u001b[39;49mtick(framerate)\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m endFunction \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     endFunction\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(endArgs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run to cell to play\n",
    "# stop the cell and run sp.stop to stop the music\n",
    "from music21 import midi, converter, instrument, note, chord\n",
    "\n",
    "mf = midi.MidiFile()\n",
    "mf.open(f\"{output_path}/music.mid\")\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "sp = midi.realtime.StreamPlayer(s)\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method StreamPlayer.stop of <music21.midi.realtime.StreamPlayer object at 0x000001F85214F160>>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
