{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mido\n",
    "!pip install pygame\n",
    "!pip install music21\n",
    "!pip install scikit-learn==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some useful libraries\n",
    "import glob, nltk, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from music21 import midi\n",
    "from plugins.midi2img import midi2img\n",
    "from plugins.img2midi import img2midi\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load In museGAN dataset for visualization purposes\n",
    "It turned out that the people at museGAN is leveraging midi -> image conversion. The image consisted of bar of a multi track piano roll. From the below image, the horizontal represent time and the vericle represent the instrument used. In this dataset the instrument are layered from bottom to top as piano, strings, guitar, drums, bass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ktrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the punkt tokenizer from nltk to tokenize the piece caption\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of museGAN\n",
    "## External Data Source\n",
    "For whatever reason, if we wanted to perform GAN modeling, we can leverage conversion of MIDI data to that of the piano roll. Download the data from piano repo in README and start performing the things below. [Convert-MIDI-TO-NP-ARRAY](https://medium.com/analytics-vidhya/convert-midi-file-to-numpy-array-in-python-7d00531890c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 20 # 18 + start, end\n",
    "EMBED_DIM = 100 \n",
    "MAX_VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>piece_id</th>\n",
       "      <th>piece_description</th>\n",
       "      <th>piece_arousal</th>\n",
       "      <th>piece_name</th>\n",
       "      <th>midi_file</th>\n",
       "      <th>caption_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>very upbeat</td>\n",
       "      <td>Delighted</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>very upbeat. delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I could tell the valence of the example was in...</td>\n",
       "      <td>Valence started out moderately negative and pr...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>i could tell the valence of the example was in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>For a second I thought this piece was going to...</td>\n",
       "      <td>This piece seemed to have a positive valence t...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>for a second i thought this piece was going to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bouncy and fun</td>\n",
       "      <td>Kind of sparatic</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>bouncy and fun. kind of sparatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>nice. nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>6100</td>\n",
       "      <td>It started off slowly but happy and then built...</td>\n",
       "      <td>Seemed to remain consistent almost like it was...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>it started off slowly but happy and then built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>6101</td>\n",
       "      <td>This starts off a certain way then changes in ...</td>\n",
       "      <td>This is nostalgic because I recognize this and...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>this starts off a certain way then changes in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6102</td>\n",
       "      <td>The piece begins slow in tempo and then become...</td>\n",
       "      <td>The beginning rhythm sounds suspenseful, makin...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the piece begins slow in tempo and then become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6103</td>\n",
       "      <td>started slow but picked up.</td>\n",
       "      <td>I feel it stayed the same</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>started slow but picked up. . i feel it stayed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>6104</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>this music sounds intense. it doesn't change m...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6105 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      piece_id                                  piece_description  \\\n",
       "0            0                                        very upbeat   \n",
       "1            1  I could tell the valence of the example was in...   \n",
       "2            2  For a second I thought this piece was going to...   \n",
       "3            3                                     Bouncy and fun   \n",
       "4            4                                               nice   \n",
       "...        ...                                                ...   \n",
       "6100      6100  It started off slowly but happy and then built...   \n",
       "6101      6101  This starts off a certain way then changes in ...   \n",
       "6102      6102  The piece begins slow in tempo and then become...   \n",
       "6103      6103                       started slow but picked up.    \n",
       "6104      6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                          piece_arousal  \\\n",
       "0                                             Delighted   \n",
       "1     Valence started out moderately negative and pr...   \n",
       "2     This piece seemed to have a positive valence t...   \n",
       "3                                      Kind of sparatic   \n",
       "4                                                  nice   \n",
       "...                                                 ...   \n",
       "6100  Seemed to remain consistent almost like it was...   \n",
       "6101  This is nostalgic because I recognize this and...   \n",
       "6102  The beginning rhythm sounds suspenseful, makin...   \n",
       "6103                         I feel it stayed the same    \n",
       "6104  this music sounds intense. it doesn't change m...   \n",
       "\n",
       "                   piece_name  \\\n",
       "0     Lurking In The Darkness   \n",
       "1     Lurking In The Darkness   \n",
       "2     Lurking In The Darkness   \n",
       "3     Lurking In The Darkness   \n",
       "4     Lurking In The Darkness   \n",
       "...                       ...   \n",
       "6100         One Winged Angel   \n",
       "6101         One Winged Angel   \n",
       "6102         One Winged Angel   \n",
       "6103         One Winged Angel   \n",
       "6104         One Winged Angel   \n",
       "\n",
       "                                              midi_file  \\\n",
       "0     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "1     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "2     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "3     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "4     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "...                                                 ...   \n",
       "6100  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6101  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6102  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6103  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6104  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "\n",
       "                                           caption_list  \n",
       "0                                very upbeat. delighted  \n",
       "1     i could tell the valence of the example was in...  \n",
       "2     for a second i thought this piece was going to...  \n",
       "3                      bouncy and fun. kind of sparatic  \n",
       "4                                            nice. nice  \n",
       "...                                                 ...  \n",
       "6100  it started off slowly but happy and then built...  \n",
       "6101  this starts off a certain way then changes in ...  \n",
       "6102  the piece begins slow in tempo and then become...  \n",
       "6103  started slow but picked up. . i feel it stayed...  \n",
       "6104  the music is kind of happy and then starts slo...  \n",
       "\n",
       "[6105 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the metadata\n",
    "# create a list of captions that concatenate the piece description and arousal\n",
    "# lower case te caption list\n",
    "midi_meta = pd.read_csv('../data/piano-labelled/labelled_piano_midi_metadata.csv')\n",
    "midi_meta['caption_list'] = midi_meta['piece_description'].str.lower()+ \". \" + midi_meta['piece_arousal'].str.lower()\n",
    "midi_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b280152c16df45c1b1268a8acd39275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a vocabulary using sklearn count vectorizer to create a vocab from the most frequent words\n",
    "input_captions = []\n",
    "max_caption_length = -1 \n",
    "\n",
    "for caption in tqdm(midi_meta['caption_list'].values):\n",
    "    tokenized_caption = nltk.word_tokenize(caption, language='english')\n",
    "\n",
    "    if len(tokenized_caption) > max_caption_length:\n",
    "        max_caption_length = len(tokenized_caption)\n",
    "\n",
    "    caption = (' '.join(tokenized_caption)).lower()\n",
    "    input_captions.append(caption)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=MAX_VOCAB_SIZE)\n",
    "vectorizer.fit(input_captions)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "MAX_VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn vocab into a dictionary of words and token id\n",
    "# replace some words with special tokens like start/end/unk\n",
    "# if the caption is too short, pad it with <pad> token\n",
    "id_vocab_dict = {}\n",
    "vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab):\n",
    "    id_vocab_dict[sid] = svocab\n",
    "    vocab_id_dict[svocab] = sid\n",
    "\n",
    "id_vocab_dict[MAX_VOCAB_SIZE] = \"<unk>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 1] = \"<start>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 2] = \"<end>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 3] = \"<pad>\"\n",
    "\n",
    "vocab_id_dict[\"<unk>\"] = MAX_VOCAB_SIZE\n",
    "vocab_id_dict[\"<start>\"] = MAX_VOCAB_SIZE + 1\n",
    "vocab_id_dict[\"<end>\"] = MAX_VOCAB_SIZE + 2\n",
    "vocab_id_dict[\"<pad>\"] = MAX_VOCAB_SIZE + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization - take the input caption and tokenize it\n",
    "# declare a max sequence length \n",
    "def convert_text_to_data(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None):\n",
    "    \"\"\"\n",
    "        Function to convert text based data into tokenized data with proper padding\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = []\n",
    "    for text_num, text in enumerate(texts):\n",
    "        sentence_ids = []\n",
    "\n",
    "        # split the sentence into token\n",
    "        # use the vocab to turn the word token into number\n",
    "        for token in text.split():\n",
    "            if token in vocab_id_dict.keys():\n",
    "                sentence_ids.append(vocab_id_dict[token])\n",
    "            else:\n",
    "                sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "\n",
    "        vocab_size = len(vocab_id_dict.keys())\n",
    "\n",
    "        # for decoder cases:\n",
    "        # input sentence: <start>, [tokenize words from vocab], <end>, padded with <unk>\n",
    "        # ouput sentence has: [tokenize words from vocab], <end>, padded with <unk>\n",
    "        if type == 'input_target':\n",
    "            ids = ([vocab_size - 3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        elif type == 'output_target':\n",
    "            ids = (sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        processed_data.append(ids)\n",
    "\n",
    "    return np.array(processed_data)\n",
    "\n",
    "\n",
    "train_target_input_data = convert_text_to_data(input_captions,\n",
    "                                                vocab_id_dict,\n",
    "                                                type='input_target',\n",
    "                                                max_length=MAX_SEQ_LENGTH)\n",
    "len(train_target_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>piece_id</th>\n",
       "      <th>piece_description</th>\n",
       "      <th>piece_arousal</th>\n",
       "      <th>piece_name</th>\n",
       "      <th>midi_file</th>\n",
       "      <th>caption_list</th>\n",
       "      <th>tokenized_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>very upbeat</td>\n",
       "      <td>Delighted</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>very upbeat. delighted</td>\n",
       "      <td>[3452, 3319, 3258, 3451, 762, 3453, 3454, 3454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I could tell the valence of the example was in...</td>\n",
       "      <td>Valence started out moderately negative and pr...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>i could tell the valence of the example was in...</td>\n",
       "      <td>[3452, 3451, 661, 3025, 3053, 3290, 2095, 3053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>For a second I thought this piece was going to...</td>\n",
       "      <td>This piece seemed to have a positive valence t...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>for a second i thought this piece was going to...</td>\n",
       "      <td>[3452, 1220, 3451, 2627, 3451, 3078, 3074, 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bouncy and fun</td>\n",
       "      <td>Kind of sparatic</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>bouncy and fun. kind of sparatic</td>\n",
       "      <td>[3452, 364, 138, 1262, 3451, 1686, 2095, 2820,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>nice. nice</td>\n",
       "      <td>[3452, 2044, 3451, 2044, 3453, 3454, 3454, 345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>6100</td>\n",
       "      <td>It started off slowly but happy and then built...</td>\n",
       "      <td>Seemed to remain consistent almost like it was...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>it started off slowly but happy and then built...</td>\n",
       "      <td>[3452, 1623, 2869, 2096, 2756, 413, 1374, 138,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>6101</td>\n",
       "      <td>This starts off a certain way then changes in ...</td>\n",
       "      <td>This is nostalgic because I recognize this and...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>this starts off a certain way then changes in ...</td>\n",
       "      <td>[3452, 3074, 2872, 2096, 3451, 468, 3366, 3061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6102</td>\n",
       "      <td>The piece begins slow in tempo and then become...</td>\n",
       "      <td>The beginning rhythm sounds suspenseful, makin...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the piece begins slow in tempo and then become...</td>\n",
       "      <td>[3452, 3053, 2241, 304, 2751, 1510, 3031, 138,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6103</td>\n",
       "      <td>started slow but picked up.</td>\n",
       "      <td>I feel it stayed the same</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>started slow but picked up. . i feel it stayed...</td>\n",
       "      <td>[3452, 2869, 2751, 413, 2233, 3451, 3451, 3451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>6104</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>this music sounds intense. it doesn't change m...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>[3452, 3053, 1996, 1618, 1686, 2095, 1374, 138...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6105 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      piece_id                                  piece_description  \\\n",
       "0            0                                        very upbeat   \n",
       "1            1  I could tell the valence of the example was in...   \n",
       "2            2  For a second I thought this piece was going to...   \n",
       "3            3                                     Bouncy and fun   \n",
       "4            4                                               nice   \n",
       "...        ...                                                ...   \n",
       "6100      6100  It started off slowly but happy and then built...   \n",
       "6101      6101  This starts off a certain way then changes in ...   \n",
       "6102      6102  The piece begins slow in tempo and then become...   \n",
       "6103      6103                       started slow but picked up.    \n",
       "6104      6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                          piece_arousal  \\\n",
       "0                                             Delighted   \n",
       "1     Valence started out moderately negative and pr...   \n",
       "2     This piece seemed to have a positive valence t...   \n",
       "3                                      Kind of sparatic   \n",
       "4                                                  nice   \n",
       "...                                                 ...   \n",
       "6100  Seemed to remain consistent almost like it was...   \n",
       "6101  This is nostalgic because I recognize this and...   \n",
       "6102  The beginning rhythm sounds suspenseful, makin...   \n",
       "6103                         I feel it stayed the same    \n",
       "6104  this music sounds intense. it doesn't change m...   \n",
       "\n",
       "                   piece_name  \\\n",
       "0     Lurking In The Darkness   \n",
       "1     Lurking In The Darkness   \n",
       "2     Lurking In The Darkness   \n",
       "3     Lurking In The Darkness   \n",
       "4     Lurking In The Darkness   \n",
       "...                       ...   \n",
       "6100         One Winged Angel   \n",
       "6101         One Winged Angel   \n",
       "6102         One Winged Angel   \n",
       "6103         One Winged Angel   \n",
       "6104         One Winged Angel   \n",
       "\n",
       "                                              midi_file  \\\n",
       "0     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "1     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "2     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "3     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "4     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "...                                                 ...   \n",
       "6100  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6101  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6102  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6103  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6104  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "\n",
       "                                           caption_list  \\\n",
       "0                                very upbeat. delighted   \n",
       "1     i could tell the valence of the example was in...   \n",
       "2     for a second i thought this piece was going to...   \n",
       "3                      bouncy and fun. kind of sparatic   \n",
       "4                                            nice. nice   \n",
       "...                                                 ...   \n",
       "6100  it started off slowly but happy and then built...   \n",
       "6101  this starts off a certain way then changes in ...   \n",
       "6102  the piece begins slow in tempo and then become...   \n",
       "6103  started slow but picked up. . i feel it stayed...   \n",
       "6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                     tokenized_captions  \n",
       "0     [3452, 3319, 3258, 3451, 762, 3453, 3454, 3454...  \n",
       "1     [3452, 3451, 661, 3025, 3053, 3290, 2095, 3053...  \n",
       "2     [3452, 1220, 3451, 2627, 3451, 3078, 3074, 224...  \n",
       "3     [3452, 364, 138, 1262, 3451, 1686, 2095, 2820,...  \n",
       "4     [3452, 2044, 3451, 2044, 3453, 3454, 3454, 345...  \n",
       "...                                                 ...  \n",
       "6100  [3452, 1623, 2869, 2096, 2756, 413, 1374, 138,...  \n",
       "6101  [3452, 3074, 2872, 2096, 3451, 468, 3366, 3061...  \n",
       "6102  [3452, 3053, 2241, 304, 2751, 1510, 3031, 138,...  \n",
       "6103  [3452, 2869, 2751, 413, 2233, 3451, 3451, 3451...  \n",
       "6104  [3452, 3053, 1996, 1618, 1686, 2095, 1374, 138...  \n",
       "\n",
       "[6105 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# added the tokenized caption to the metadata\n",
    "midi_meta['tokenized_captions'] = train_target_input_data.tolist()\n",
    "midi_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN Definition\n",
    "\n",
    "Enhancement to the original GAN model where the Wasserstein distance is leverage to produce a value function that has better theoretical properties than orginal GAN.\n",
    "GAN model consists of two part:\n",
    "1. Generator\n",
    "2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# check to see if tensorflow mount to GPU properly\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_generator(latent_dim=100, \n",
    "                               caption_dim=MAX_SEQ_LENGTH, \n",
    "                               vocab_size=len(vocab_id_dict.keys()), \n",
    "                               embed_dim=EMBED_DIM):\n",
    "    \"\"\"Define the generator model\n",
    "        Inputs:\n",
    "            latent_dim: dimension of the latent space\n",
    "        Output:\n",
    "            model: the generator model\n",
    "    \"\"\"\n",
    "    n_nodes = 128 * 53 * 53\n",
    "\n",
    "    # vectorized input layers\n",
    "    input_layer = keras.layers.Input(shape=(latent_dim,), name='input_layer')\n",
    "    \n",
    "    # # vectorized caption input layers\n",
    "    # # apply word embedding to the caption\n",
    "    caption_input_layer = keras.layers.Input(shape=(caption_dim,), name='caption_input_layer')\n",
    "    embedding_layer  = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=embed_dim,\n",
    "                                                name='caption_embedding_layer')\n",
    "    embed_caption = embedding_layer(caption_input_layer)\n",
    "\n",
    "    # # source_image_encoding = keras.layers.GlobalAveragePooling2D()(dense4)\n",
    "    # # using LSTM to encode the caption with the input layer\n",
    "    lstm_layer = keras.layers.LSTM(100, return_sequences=True, return_state=True, name=\"decoder_lstm_layer\")\n",
    "    decoder_output, decoder_state_h_output, decoder_state_c_output = lstm_layer(embed_caption, initial_state=[input_layer, input_layer])\n",
    "\n",
    "    # apply 1D Global Average Pooling to the output of the dense layer on the caption decoded\n",
    "    # global_average_pooling1d_layer = keras.layers.GlobalAveragePooling1D()(decoder_output)\n",
    "\n",
    "    # Dense Layer 1\n",
    "    dense1 = keras.layers.Dense(n_nodes, activation='relu')(decoder_state_c_output)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.3)(dense1)\n",
    "    reshape_layer = keras.layers.Reshape((53, 53, 128))(leaky_relu1)\n",
    "\n",
    "    # Dense Layer 2\n",
    "    dense2 =  keras.layers.Dense(1024)(reshape_layer)\n",
    "\n",
    "    # Conv2DTranspose Layer\n",
    "    conv2d_transpose = keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same')(dense2)\n",
    "\n",
    "    # Dense Layer 3\n",
    "    dense3 =  keras.layers.Dense(1024)(conv2d_transpose)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.3)(dense3)\n",
    "\n",
    "    # Dense Layer 4\n",
    "    dense4 =  keras.layers.Dense(1024)(leaky_relu2)\n",
    "\n",
    "    # Conv2D Layer\n",
    "    conv2d = keras.layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')(dense4)\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[input_layer,caption_input_layer], outputs=conv2d, name='generator')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = keras.layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "def caption_enhanced_discriminator(in_shape = (106,106,1)):\n",
    "    \"\"\"\n",
    "        GAN discriminator model\n",
    "        Inputs:\n",
    "            in_shape: shape of the input image\n",
    "        Output:\n",
    "            model: discriminator model with binary crossentropy loss to denotes if the image is real or fake\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = keras.layers.Input(shape=in_shape, name='input_layer')\n",
    "    \n",
    "     \n",
    "    # 2D Convlution Layer 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(input_layer)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    dropout1 = keras.layers.Dropout(0.5)(leaky_relu1)\n",
    "\n",
    "    # 2D Convlution Layer 2\n",
    "    conv2 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(dropout1)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    dropout2 = keras.layers.Dropout(0.5)(leaky_relu2)\n",
    "\n",
    "    # Flatten Layer\n",
    "    flatten_layer = keras.layers.Flatten()(dropout2)\n",
    "\n",
    "    # Batch Normalization Layer\n",
    "    batch_normalization = keras.layers.BatchNormalization()(flatten_layer)\n",
    "    \n",
    "    # Dense Output Disminator Layer\n",
    "    discriminate_layer = keras.layers.Dense(1)(batch_normalization)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=input_layer, outputs=discriminate_layer, name='discriminator')\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image by adding a random noise the the residuals\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "    \n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        # Unpack the data\n",
    "        real_images, real_captions = dataset\n",
    "\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128, filepath=\".\"):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        # generated_images = self.model.generator(random_latent_vectors)\n",
    "        # generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        # for i in range(self.num_img):\n",
    "        #     img = generated_images[i].numpy()\n",
    "        #     img = keras.utils.array_to_img(img)\n",
    "        #     img.save(f\"{self.filepath}/generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "\n",
    "\n",
    "        # save the model \n",
    "        self.model.generator.save(f\"{self.filepath}/generator_{epoch}.h5\".format(epoch=epoch))\n",
    "        self.model.discriminator.save(f\"{self.filepath}/discriminator_{epoch}.h5\".format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input_layer (InputLayer [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embedding_layer (Embedd (None, 20, 100)      345500      caption_input_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_layer (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_layer (LSTM)       [(None, 20, 100), (N 80400       caption_embedding_layer[0][0]    \n",
      "                                                                 input_layer[0][0]                \n",
      "                                                                 input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 359552)       36314752    decoder_lstm_layer[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 359552)       0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 53, 53, 128)  0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 53, 53, 1024) 132096      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 106, 106, 102 16778240    dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 106, 106, 102 1049600     conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 106, 106, 102 0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 106, 106, 102 1049600     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 106, 106, 1)  50177       dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 55,800,365\n",
      "Trainable params: 55,800,365\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 106, 106, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 53, 53, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 46656)             186624    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 46657     \n",
      "=================================================================\n",
      "Total params: 270,849\n",
      "Trainable params: 177,537\n",
      "Non-trainable params: 93,312\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_model = caption_enhanced_generator()\n",
    "d_model = caption_enhanced_discriminator()\n",
    "g_model.summary(), d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "import random\n",
    "random.seed(5634)\n",
    "\n",
    "training_set = joblib.load('../data/piano-labelled/training_set.joblib')\n",
    "training_set = random.sample(training_set, 2048) #5 is the lenth of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (2048, 106, 106, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unzip the dataset\n",
    "images, captions = zip(*training_set)\n",
    "images = np.array(images)\n",
    "captions = np.array(captions)\n",
    "type(images), images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "256/256 [==============================] - 946s 4s/step - d_loss: 10.0000 - g_loss: 5.3343e-09\n",
      "Epoch 2/3\n",
      "256/256 [==============================] - 949s 4s/step - d_loss: 10.0000 - g_loss: 5.0262e-09\n",
      "Epoch 3/3\n",
      "150/256 [================>.............] - ETA: 6:09 - d_loss: 10.0000 - g_loss: 3.0547e-09"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museWGAN_experiment.ipynb Cell 22\u001b[0m line \u001b[0;36m<cell line: 53>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m wgan\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     d_optimizer\u001b[39m=\u001b[39mdiscriminator_optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     g_optimizer\u001b[39m=\u001b[39mgenerator_optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     g_loss_fn\u001b[39m=\u001b[39mgenerator_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     d_loss_fn\u001b[39m=\u001b[39mdiscriminator_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museWGAN_experiment.ipynb#X30sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m wgan\u001b[39m.\u001b[39;49mfit(images, captions, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set some training parameters\n",
    "BATCH_SIZE = 8\n",
    "noise_dim = 100\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9,\n",
    "    clipnorm=2.0\n",
    ")\n",
    "\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9,\n",
    "    clipnorm=2.0\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(num_img=3, latent_dim=noise_dim, filepath='../model/wgan-callbacks')\n",
    "\n",
    "# Get the wgan model\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "wgan.fit(images, captions, batch_size=BATCH_SIZE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "g_model.save('../models/mini-gan/caption_piano_wgan_generator_model.h5')\n",
    "d_model.save('../models/mini-gan/caption_piano_wgan_discriminator_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# g_model = keras.models.load_model('../models/mini-gan/caption_piano_wgan_generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['it goes from forced to worried . determination'], dtype=object),\n",
       " array([[3452, 1623, 1307, 1253, 1223, 3122, 3423, 3451,  800, 3453, 3454,\n",
       "         3454, 3454, 3454, 3454, 3454, 3454, 3454, 3454, 3454]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random row from the metadata to get the caption\n",
    "row = midi_meta.sample(1, random_state=63)\n",
    "\n",
    "# get a random image tokenize caption and actual caption\n",
    "NLP_caption = row['caption_list'].values\n",
    "caption = [np.array(a) for a in row['tokenized_captions'].values]\n",
    "caption = np.array(caption)\n",
    "NLP_caption, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (1, 20))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "\n",
    "# declare a latent space\n",
    "latent_dim = 100\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "latent_points.shape, caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = g_model\n",
    "X = g_model.predict([latent_points, caption])\n",
    "array = np.array(X.reshape(106,106),dtype = np.uint8)\n",
    "np.unique(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "array*=255\n",
    "new_image = Image.fromarray(array,'L')\n",
    "new_image = new_image.save(f'../data/midi_reconstruction/images/wgan_captioned_piece_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconvert MIDI images to MIDI files\n",
    "image_path = \"../data/midi_reconstruction/images/wgan_captioned_piece_test.png\"\n",
    "output_path = \"../data/midi_reconstruction\"\n",
    "\n",
    "img2midi_obj = img2midi(image_path, output_path, resolution=0.25)\n",
    "img2midi_obj.convert_to_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to cell to play\n",
    "# stop the cell and run sp.stop to stop the music\n",
    "from music21 import midi, converter, instrument, note, chord\n",
    "\n",
    "mf = midi.MidiFile()\n",
    "mf.open(f\"{output_path}/music.mid\")\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "sp = midi.realtime.StreamPlayer(s)\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
