{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mido in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: packaging~=23.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from mido) (23.1)\n",
      "Requirement already satisfied: pygame in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: music21 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (7.3.3)\n",
      "Requirement already satisfied: webcolors>=1.5 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.12)\n",
      "Requirement already satisfied: chardet in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (5.0.0)\n",
      "Requirement already satisfied: jsonpickle in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (3.5.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.3.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (8.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ktrin\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mido\n",
    "!pip install pygame\n",
    "!pip install music21\n",
    "!pip install scikit-learn==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some useful libraries\n",
    "import glob, nltk, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from music21 import midi\n",
    "from plugins.midi2img import midi2img\n",
    "from plugins.img2midi import img2midi\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load In museGAN dataset for visualization purposes\n",
    "It turned out that the people at museGAN is leveraging midi -> image conversion. The image consisted of bar of a multi track piano roll. From the below image, the horizontal represent time and the vericle represent the instrument used. In this dataset the instrument are layered from bottom to top as piano, strings, guitar, drums, bass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ktrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the punkt tokenizer from nltk to tokenize the piece caption\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of museGAN\n",
    "## External Data Source\n",
    "For whatever reason, if we wanted to perform GAN modeling, we can leverage conversion of MIDI data to that of the piano roll. Download the data from piano repo in README and start performing the things below. [Convert-MIDI-TO-NP-ARRAY](https://medium.com/analytics-vidhya/convert-midi-file-to-numpy-array-in-python-7d00531890c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 20 # 18 + start, end\n",
    "EMBED_DIM = 100 \n",
    "MAX_VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>piece_id</th>\n",
       "      <th>piece_description</th>\n",
       "      <th>piece_arousal</th>\n",
       "      <th>piece_name</th>\n",
       "      <th>midi_file</th>\n",
       "      <th>caption_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>very upbeat</td>\n",
       "      <td>Delighted</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>very upbeat. delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I could tell the valence of the example was in...</td>\n",
       "      <td>Valence started out moderately negative and pr...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>i could tell the valence of the example was in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>For a second I thought this piece was going to...</td>\n",
       "      <td>This piece seemed to have a positive valence t...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>for a second i thought this piece was going to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bouncy and fun</td>\n",
       "      <td>Kind of sparatic</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>bouncy and fun. kind of sparatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>nice. nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>6100</td>\n",
       "      <td>It started off slowly but happy and then built...</td>\n",
       "      <td>Seemed to remain consistent almost like it was...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>it started off slowly but happy and then built...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>6101</td>\n",
       "      <td>This starts off a certain way then changes in ...</td>\n",
       "      <td>This is nostalgic because I recognize this and...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>this starts off a certain way then changes in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6102</td>\n",
       "      <td>The piece begins slow in tempo and then become...</td>\n",
       "      <td>The beginning rhythm sounds suspenseful, makin...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the piece begins slow in tempo and then become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6103</td>\n",
       "      <td>started slow but picked up.</td>\n",
       "      <td>I feel it stayed the same</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>started slow but picked up. . i feel it stayed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>6104</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>this music sounds intense. it doesn't change m...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6105 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      piece_id                                  piece_description  \\\n",
       "0            0                                        very upbeat   \n",
       "1            1  I could tell the valence of the example was in...   \n",
       "2            2  For a second I thought this piece was going to...   \n",
       "3            3                                     Bouncy and fun   \n",
       "4            4                                               nice   \n",
       "...        ...                                                ...   \n",
       "6100      6100  It started off slowly but happy and then built...   \n",
       "6101      6101  This starts off a certain way then changes in ...   \n",
       "6102      6102  The piece begins slow in tempo and then become...   \n",
       "6103      6103                       started slow but picked up.    \n",
       "6104      6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                          piece_arousal  \\\n",
       "0                                             Delighted   \n",
       "1     Valence started out moderately negative and pr...   \n",
       "2     This piece seemed to have a positive valence t...   \n",
       "3                                      Kind of sparatic   \n",
       "4                                                  nice   \n",
       "...                                                 ...   \n",
       "6100  Seemed to remain consistent almost like it was...   \n",
       "6101  This is nostalgic because I recognize this and...   \n",
       "6102  The beginning rhythm sounds suspenseful, makin...   \n",
       "6103                         I feel it stayed the same    \n",
       "6104  this music sounds intense. it doesn't change m...   \n",
       "\n",
       "                   piece_name  \\\n",
       "0     Lurking In The Darkness   \n",
       "1     Lurking In The Darkness   \n",
       "2     Lurking In The Darkness   \n",
       "3     Lurking In The Darkness   \n",
       "4     Lurking In The Darkness   \n",
       "...                       ...   \n",
       "6100         One Winged Angel   \n",
       "6101         One Winged Angel   \n",
       "6102         One Winged Angel   \n",
       "6103         One Winged Angel   \n",
       "6104         One Winged Angel   \n",
       "\n",
       "                                              midi_file  \\\n",
       "0     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "1     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "2     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "3     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "4     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "...                                                 ...   \n",
       "6100  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6101  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6102  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6103  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6104  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "\n",
       "                                           caption_list  \n",
       "0                                very upbeat. delighted  \n",
       "1     i could tell the valence of the example was in...  \n",
       "2     for a second i thought this piece was going to...  \n",
       "3                      bouncy and fun. kind of sparatic  \n",
       "4                                            nice. nice  \n",
       "...                                                 ...  \n",
       "6100  it started off slowly but happy and then built...  \n",
       "6101  this starts off a certain way then changes in ...  \n",
       "6102  the piece begins slow in tempo and then become...  \n",
       "6103  started slow but picked up. . i feel it stayed...  \n",
       "6104  the music is kind of happy and then starts slo...  \n",
       "\n",
       "[6105 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the metadata\n",
    "# create a list of captions that concatenate the piece description and arousal\n",
    "# lower case te caption list\n",
    "midi_meta = pd.read_csv('../data/piano-labelled/labelled_piano_midi_metadata.csv')\n",
    "midi_meta['caption_list'] = midi_meta['piece_description'].str.lower()+ \". \" + midi_meta['piece_arousal'].str.lower()\n",
    "midi_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfc8c889ae44842a2bc48d4eee6a4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a vocabulary using sklearn count vectorizer to create a vocab from the most frequent words\n",
    "input_captions = []\n",
    "max_caption_length = -1 \n",
    "\n",
    "for caption in tqdm(midi_meta['caption_list'].values):\n",
    "    tokenized_caption = nltk.word_tokenize(caption, language='english')\n",
    "\n",
    "    if len(tokenized_caption) > max_caption_length:\n",
    "        max_caption_length = len(tokenized_caption)\n",
    "\n",
    "    caption = (' '.join(tokenized_caption)).lower()\n",
    "    input_captions.append(caption)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=MAX_VOCAB_SIZE)\n",
    "vectorizer.fit(input_captions)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "MAX_VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn vocab into a dictionary of words and token id\n",
    "# replace some words with special tokens like start/end/unk\n",
    "# if the caption is too short, pad it with <pad> token\n",
    "id_vocab_dict = {}\n",
    "vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab):\n",
    "    id_vocab_dict[sid] = svocab\n",
    "    vocab_id_dict[svocab] = sid\n",
    "\n",
    "id_vocab_dict[MAX_VOCAB_SIZE] = \"<unk>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 1] = \"<start>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 2] = \"<end>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 3] = \"<pad>\"\n",
    "\n",
    "vocab_id_dict[\"<unk>\"] = MAX_VOCAB_SIZE\n",
    "vocab_id_dict[\"<start>\"] = MAX_VOCAB_SIZE + 1\n",
    "vocab_id_dict[\"<end>\"] = MAX_VOCAB_SIZE + 2\n",
    "vocab_id_dict[\"<pad>\"] = MAX_VOCAB_SIZE + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization - take the input caption and tokenize it\n",
    "# declare a max sequence length \n",
    "def convert_text_to_data(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None):\n",
    "    \"\"\"\n",
    "        Function to convert text based data into tokenized data with proper padding\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = []\n",
    "    for text_num, text in enumerate(texts):\n",
    "        sentence_ids = []\n",
    "\n",
    "        # split the sentence into token\n",
    "        # use the vocab to turn the word token into number\n",
    "        for token in text.split():\n",
    "            if token in vocab_id_dict.keys():\n",
    "                sentence_ids.append(vocab_id_dict[token])\n",
    "            else:\n",
    "                sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "\n",
    "        vocab_size = len(vocab_id_dict.keys())\n",
    "\n",
    "        # for decoder cases:\n",
    "        # input sentence: <start>, [tokenize words from vocab], <end>, padded with <unk>\n",
    "        # ouput sentence has: [tokenize words from vocab], <end>, padded with <unk>\n",
    "        if type == 'input_target':\n",
    "            ids = ([vocab_size - 3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        elif type == 'output_target':\n",
    "            ids = (sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        processed_data.append(ids)\n",
    "\n",
    "    return np.array(processed_data)\n",
    "\n",
    "\n",
    "train_target_input_data = convert_text_to_data(input_captions,\n",
    "                                                vocab_id_dict,\n",
    "                                                type='input_target',\n",
    "                                                max_length=MAX_SEQ_LENGTH)\n",
    "len(train_target_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>piece_id</th>\n",
       "      <th>piece_description</th>\n",
       "      <th>piece_arousal</th>\n",
       "      <th>piece_name</th>\n",
       "      <th>midi_file</th>\n",
       "      <th>caption_list</th>\n",
       "      <th>tokenized_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>very upbeat</td>\n",
       "      <td>Delighted</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>very upbeat. delighted</td>\n",
       "      <td>[3452, 3319, 3258, 3451, 762, 3453, 3454, 3454...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I could tell the valence of the example was in...</td>\n",
       "      <td>Valence started out moderately negative and pr...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>i could tell the valence of the example was in...</td>\n",
       "      <td>[3452, 3451, 661, 3025, 3053, 3290, 2095, 3053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>For a second I thought this piece was going to...</td>\n",
       "      <td>This piece seemed to have a positive valence t...</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>for a second i thought this piece was going to...</td>\n",
       "      <td>[3452, 1220, 3451, 2627, 3451, 3078, 3074, 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bouncy and fun</td>\n",
       "      <td>Kind of sparatic</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>bouncy and fun. kind of sparatic</td>\n",
       "      <td>[3452, 364, 138, 1262, 3451, 1686, 2095, 2820,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>Lurking In The Darkness</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_Lurking In...</td>\n",
       "      <td>nice. nice</td>\n",
       "      <td>[3452, 2044, 3451, 2044, 3453, 3454, 3454, 345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>6100</td>\n",
       "      <td>It started off slowly but happy and then built...</td>\n",
       "      <td>Seemed to remain consistent almost like it was...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>it started off slowly but happy and then built...</td>\n",
       "      <td>[3452, 1623, 2869, 2096, 2756, 413, 1374, 138,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>6101</td>\n",
       "      <td>This starts off a certain way then changes in ...</td>\n",
       "      <td>This is nostalgic because I recognize this and...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>this starts off a certain way then changes in ...</td>\n",
       "      <td>[3452, 3074, 2872, 2096, 3451, 468, 3366, 3061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>6102</td>\n",
       "      <td>The piece begins slow in tempo and then become...</td>\n",
       "      <td>The beginning rhythm sounds suspenseful, makin...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the piece begins slow in tempo and then become...</td>\n",
       "      <td>[3452, 3053, 2241, 304, 2751, 1510, 3031, 138,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6103</td>\n",
       "      <td>started slow but picked up.</td>\n",
       "      <td>I feel it stayed the same</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>started slow but picked up. . i feel it stayed...</td>\n",
       "      <td>[3452, 2869, 2751, 413, 2233, 3451, 3451, 3451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>6104</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>this music sounds intense. it doesn't change m...</td>\n",
       "      <td>One Winged Angel</td>\n",
       "      <td>Final Fantasy_PS1_Final Fantasy VII_One Winged...</td>\n",
       "      <td>the music is kind of happy and then starts slo...</td>\n",
       "      <td>[3452, 3053, 1996, 1618, 1686, 2095, 1374, 138...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      piece_id                                  piece_description  \\\n",
       "0            0                                        very upbeat   \n",
       "1            1  I could tell the valence of the example was in...   \n",
       "2            2  For a second I thought this piece was going to...   \n",
       "3            3                                     Bouncy and fun   \n",
       "4            4                                               nice   \n",
       "...        ...                                                ...   \n",
       "6100      6100  It started off slowly but happy and then built...   \n",
       "6101      6101  This starts off a certain way then changes in ...   \n",
       "6102      6102  The piece begins slow in tempo and then become...   \n",
       "6103      6103                       started slow but picked up.    \n",
       "6104      6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                          piece_arousal  \\\n",
       "0                                             Delighted   \n",
       "1     Valence started out moderately negative and pr...   \n",
       "2     This piece seemed to have a positive valence t...   \n",
       "3                                      Kind of sparatic   \n",
       "4                                                  nice   \n",
       "...                                                 ...   \n",
       "6100  Seemed to remain consistent almost like it was...   \n",
       "6101  This is nostalgic because I recognize this and...   \n",
       "6102  The beginning rhythm sounds suspenseful, makin...   \n",
       "6103                         I feel it stayed the same    \n",
       "6104  this music sounds intense. it doesn't change m...   \n",
       "\n",
       "                   piece_name  \\\n",
       "0     Lurking In The Darkness   \n",
       "1     Lurking In The Darkness   \n",
       "2     Lurking In The Darkness   \n",
       "3     Lurking In The Darkness   \n",
       "4     Lurking In The Darkness   \n",
       "...                       ...   \n",
       "6100         One Winged Angel   \n",
       "6101         One Winged Angel   \n",
       "6102         One Winged Angel   \n",
       "6103         One Winged Angel   \n",
       "6104         One Winged Angel   \n",
       "\n",
       "                                              midi_file  \\\n",
       "0     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "1     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "2     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "3     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "4     Final Fantasy_PS1_Final Fantasy VII_Lurking In...   \n",
       "...                                                 ...   \n",
       "6100  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6101  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6102  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6103  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "6104  Final Fantasy_PS1_Final Fantasy VII_One Winged...   \n",
       "\n",
       "                                           caption_list  \\\n",
       "0                                very upbeat. delighted   \n",
       "1     i could tell the valence of the example was in...   \n",
       "2     for a second i thought this piece was going to...   \n",
       "3                      bouncy and fun. kind of sparatic   \n",
       "4                                            nice. nice   \n",
       "...                                                 ...   \n",
       "6100  it started off slowly but happy and then built...   \n",
       "6101  this starts off a certain way then changes in ...   \n",
       "6102  the piece begins slow in tempo and then become...   \n",
       "6103  started slow but picked up. . i feel it stayed...   \n",
       "6104  the music is kind of happy and then starts slo...   \n",
       "\n",
       "                                     tokenized_captions  \n",
       "0     [3452, 3319, 3258, 3451, 762, 3453, 3454, 3454...  \n",
       "1     [3452, 3451, 661, 3025, 3053, 3290, 2095, 3053...  \n",
       "2     [3452, 1220, 3451, 2627, 3451, 3078, 3074, 224...  \n",
       "3     [3452, 364, 138, 1262, 3451, 1686, 2095, 2820,...  \n",
       "4     [3452, 2044, 3451, 2044, 3453, 3454, 3454, 345...  \n",
       "...                                                 ...  \n",
       "6100  [3452, 1623, 2869, 2096, 2756, 413, 1374, 138,...  \n",
       "6101  [3452, 3074, 2872, 2096, 3451, 468, 3366, 3061...  \n",
       "6102  [3452, 3053, 2241, 304, 2751, 1510, 3031, 138,...  \n",
       "6103  [3452, 2869, 2751, 413, 2233, 3451, 3451, 3451...  \n",
       "6104  [3452, 3053, 1996, 1618, 1686, 2095, 1374, 138...  \n",
       "\n",
       "[6105 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# added the tokenized caption to the metadata\n",
    "midi_meta['tokenized_captions'] = train_target_input_data.tolist()\n",
    "midi_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['the piece started out rather uninteresting. however, i became more interested when it took on a new feel, and  more excited.. the beginning was kind of harsh, but i liked it when it became less so later. that decreased my arousal.'],\n",
       "       dtype=object),\n",
       " array([[3452, 3053, 2241, 2869, 2141, 2428, 3241, 3451, 1467, 3451, 3451,\n",
       "          290, 1970, 1579, 3383, 1623, 3138, 2110, 3451, 2041]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # select a random row from the metadata to get the caption\n",
    "# row = midi_meta.sample(1, random_state=22)\n",
    "\n",
    "# # get a random image tokenize caption and actual caption\n",
    "# NLP_caption = row['caption_list'].values\n",
    "# caption = [np.array(a) for a in row['tokenized_captions'].values]\n",
    "# caption = np.array(caption)\n",
    "# NLP_caption, caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Definition\n",
    "\n",
    "GAN model consists of two part:\n",
    "1. Generator\n",
    "2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# check to see if tensorflow mount to GPU properly\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_generator(latent_dim=100, \n",
    "                               caption_dim=MAX_SEQ_LENGTH, \n",
    "                               vocab_size=len(vocab_id_dict.keys()), \n",
    "                               embed_dim=EMBED_DIM):\n",
    "    \"\"\"Define the generator model\n",
    "        Inputs:\n",
    "            latent_dim: dimension of the latent space\n",
    "        Output:\n",
    "            model: the generator model\n",
    "    \"\"\"\n",
    "    n_nodes = 128 * 53 * 53\n",
    "\n",
    "    # vectorized input layers\n",
    "    input_layer = keras.layers.Input(shape=(latent_dim,), name='input_layer')\n",
    "    \n",
    "    # # vectorized caption input layers\n",
    "    # # apply word embedding to the caption\n",
    "    caption_input_layer = keras.layers.Input(shape=(caption_dim,), name='caption_input_layer')\n",
    "    embedding_layer  = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=embed_dim,\n",
    "                                                name='caption_embedding_layer')\n",
    "    embed_caption = embedding_layer(caption_input_layer)\n",
    "\n",
    "    # # source_image_encoding = keras.layers.GlobalAveragePooling2D()(dense4)\n",
    "    # # using LSTM to encode the caption with the input layer\n",
    "    lstm_layer = keras.layers.LSTM(100, return_sequences=True, return_state=True, name=\"decoder_lstm_layer\")\n",
    "    decoder_output, decoder_state_h_output, decoder_state_c_output = lstm_layer(embed_caption, initial_state=[input_layer, input_layer])\n",
    "\n",
    "    # apply 1D Global Average Pooling to the output of the dense layer on the caption decoded\n",
    "    # global_average_pooling1d_layer = keras.layers.GlobalAveragePooling1D()(decoder_output)\n",
    "\n",
    "    # Dense Layer 1\n",
    "    dense1 = keras.layers.Dense(n_nodes)(decoder_state_c_output)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.35)(dense1)\n",
    "    reshape_layer = keras.layers.Reshape((53, 53, 128))(leaky_relu1)\n",
    "\n",
    "    # Dense Layer 2\n",
    "    dense2 =  keras.layers.Dense(1024)(reshape_layer)\n",
    "\n",
    "    # Conv2DTranspose Layer\n",
    "    conv2d_transpose = keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same')(dense2)\n",
    "\n",
    "    # Dense Layer 3\n",
    "    dense3 =  keras.layers.Dense(1024)(conv2d_transpose)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.35)(dense3)\n",
    "\n",
    "    # Dense Layer 4\n",
    "    dense4 =  keras.layers.Dense(1024)(leaky_relu2)\n",
    "\n",
    "    # Conv2D Layer\n",
    "    conv2d = keras.layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')(dense4)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[input_layer,caption_input_layer], outputs=conv2d, name='generator_model')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_discriminator(in_shape = (106,106,1)):\n",
    "    \"\"\"\n",
    "        GAN discriminator model\n",
    "        Inputs:\n",
    "            in_shape: shape of the input image\n",
    "        Output:\n",
    "            model: discriminator model with binary crossentropy loss to denotes if the image is real or fake\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = keras.layers.Input(shape=in_shape, name='input_layer')\n",
    "    \n",
    "    # 2D Convlution Layer 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(input_layer)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    dropout1 = keras.layers.Dropout(0.5)(leaky_relu1)\n",
    "\n",
    "    # 2D Convlution Layer 2\n",
    "    conv2 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(dropout1)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    dropout2 = keras.layers.Dropout(0.5)(leaky_relu2)\n",
    "\n",
    "    # Flatten Layer\n",
    "    flatten_layer = keras.layers.Flatten()(dropout2)\n",
    "\n",
    "    # Batch Normalization Layer\n",
    "    batch_normalization = keras.layers.BatchNormalization()(flatten_layer)\n",
    "\n",
    "    # Dense Output Disminator Layer\n",
    "    discriminate_layer = keras.layers.Dense(1, activation='sigmoid')(batch_normalization)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=input_layer, outputs=discriminate_layer, name='discriminator_model')\n",
    "    \n",
    "    # model compile\n",
    "    # opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_miniGAN(g_model, d_model,\n",
    "                             g_model_input_shape=100, \n",
    "                             g_model_caption_input_shape=MAX_SEQ_LENGTH):\n",
    "    \"\"\"\n",
    "        GAN model architecture\n",
    "        Inputs:\n",
    "            g_model: generator model\n",
    "            d_model: discriminator model\n",
    "            g_model_input_shape: shape of the input to the generator model\n",
    "            g_model_caption_input_shape: shape of the input caption to the generator model\n",
    "        Output:\n",
    "            model: GAN model\n",
    "    \"\"\"\n",
    "    # Pause the training of the discriminator\n",
    "    d_model.trainable = False\n",
    "\n",
    "    # Define the input layer for the generator\n",
    "    generator_input = keras.layers.Input(shape=(g_model_input_shape))  # Specify the shape of the generator's input\n",
    "    caption_input = keras.layers.Input(shape=(g_model_caption_input_shape))  # Specify the shape of the generator's input\n",
    "\n",
    "    # Define the output of the generator\n",
    "    generator_output = g_model([generator_input, caption_input])\n",
    "\n",
    "    # Define the output of the discriminator\n",
    "    discriminator_output = d_model(generator_output)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[generator_input, caption_input], outputs=discriminator_output)\n",
    "\n",
    "    # # Compile the model\n",
    "    # opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, beta_2=0.9, clipnorm=1.0)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        gan,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.d_gradient = []\n",
    "        self.g_gradient = []\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "\n",
    "    # def generate_real_samples(dataset, n_samples):\n",
    "    #     # slice the image and caption from the dataset\n",
    "    #     # generate 'real' class labels (1)\n",
    "    #     images, captions = zip(*dataset)\n",
    "    #     images = np.array(images)\n",
    "    #     captions = np.array(captions)\n",
    "    #     ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    #     X_img = images[ix]   \n",
    "    #     X_cap = captions[ix]\n",
    "    #     y = np.ones((n_samples, 1))\n",
    "    #     return X_img, X_cap, y\n",
    "    \n",
    "    # def generate_latent_points(latent_dim, n_samples):\n",
    "    #     x_input = np.random.randn(latent_dim * n_samples)\n",
    "    #     x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    #     return x_input\n",
    "\n",
    "    # def generate_fake_samples(g_model, latent_dim, caption, n_samples):\n",
    "    #     x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    #     X = g_model.predict([x_input, caption])\n",
    "    #     y = np.zeros((n_samples, 1))\n",
    "    #     return X, y\n",
    "\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        # Unpack the data\n",
    "        real_images, real_captions = dataset\n",
    "\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            # Get the latent vector (random noise)\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                \n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_loss = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_gradient.append((i, d_gradient))\n",
    "\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        self.g_gradient.append(gen_gradient)\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # slice the image and caption from the dataset\n",
    "    # generate 'real' class labels (1)\n",
    "    images, captions = zip(*dataset)\n",
    "    images = np.array(images)\n",
    "    captions = np.array(captions)\n",
    "    ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    X_img = images[ix]   \n",
    "    X_cap = captions[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X_img, X_cap, y\n",
    " \n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(g_model, latent_dim, caption, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = g_model.predict([x_input, caption])\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input_layer (InputLayer [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embedding_layer (Embedd (None, 20, 100)      345500      caption_input_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_layer (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_layer (LSTM)       [(None, 20, 100), (N 80400       caption_embedding_layer[0][0]    \n",
      "                                                                 input_layer[0][0]                \n",
      "                                                                 input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 359552)       36314752    decoder_lstm_layer[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 359552)       0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 53, 53, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 53, 53, 1024) 132096      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 106, 106, 102 16778240    dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 106, 106, 102 1049600     conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 106, 106, 102 0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 106, 106, 102 1049600     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 106, 106, 1)  50177       dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 55,800,365\n",
      "Trainable params: 55,800,365\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 106, 106, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 53, 53, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 46656)             186624    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 46657     \n",
      "=================================================================\n",
      "Total params: 270,849\n",
      "Trainable params: 177,537\n",
      "Non-trainable params: 93,312\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "g_model = caption_enhanced_generator(latent_dim)\n",
    "d_model = caption_enhanced_discriminator()\n",
    "g_model.summary(), d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(g_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(d_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(gan_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "import random\n",
    "random.seed(5634)\n",
    "\n",
    "training_set = joblib.load('../data/piano-labelled/training_set.joblib')\n",
    "training_set = random.sample(training_set, 2048) #5 is the lenth of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (2048, 106, 106, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unzip the dataset\n",
    "images, captions = zip(*training_set)\n",
    "images = np.array(images)\n",
    "captions = np.array(captions)\n",
    "type(images), images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[67416,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node generator_model/dense_7/Tensordot_1/MatMul (defined at C:\\Users\\ktrin\\AppData\\Local\\Temp\\ipykernel_22320\\1020710438.py:58) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_68615]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museGAN_experiment_w_caption_v2.ipynb Cell 25\u001b[0m line \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m gan\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     d_optimizer\u001b[39m=\u001b[39mdiscriminator_optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     g_optimizer\u001b[39m=\u001b[39mgenerator_optimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     g_loss_fn\u001b[39m=\u001b[39mgenerator_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     d_loss_fn\u001b[39m=\u001b[39mdiscriminator_loss,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#Y112sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m gan\u001b[39m.\u001b[39;49mfit(images, captions, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[1;32mc:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1185\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:950\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    947\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 950\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    951\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m   _, _, _, filtered_flat_args \u001b[39m=\u001b[39m \\\n\u001b[0;32m    953\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    954\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[67416,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node generator_model/dense_7/Tensordot_1/MatMul (defined at C:\\Users\\ktrin\\AppData\\Local\\Temp\\ipykernel_22320\\1020710438.py:58) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_68615]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Set some training parameters\n",
    "BATCH_SIZE = 6\n",
    "noise_dim = 100\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "# cbk = GANMonitor(num_img=3, latent_dim=noise_dim, filepath='../model/wgan-callbacks')\n",
    "\n",
    "# Get the wgan model\n",
    "gan = GAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "gan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "gan.fit(images, captions, batch_size=BATCH_SIZE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "g_model.save('../models/mini-gan/caption_piano_generator_model_g3.h5')\n",
    "d_model.save('../models/mini-gan/caption_piano_discriminator_model_g3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "g_model = keras.models.load_model('../models/mini-gan/caption_piano_generator_model_g3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['the piece started out rather uninteresting. however, i became more interested when it took on a new feel, and  more excited.. the beginning was kind of harsh, but i liked it when it became less so later. that decreased my arousal.'],\n",
       "       dtype=object),\n",
       " array([[3452, 3053, 2241, 2869, 2141, 2428, 3241, 3451, 1467, 3451, 3451,\n",
       "          290, 1970, 1579, 3383, 1623, 3138, 2110, 3451, 2041]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random row from the metadata to get the caption\n",
    "row = midi_meta.sample(1, random_state=22)\n",
    "\n",
    "# get a random image tokenize caption and actual caption\n",
    "NLP_caption = row['caption_list'].values\n",
    "caption = [np.array(a) for a in row['tokenized_captions'].values]\n",
    "caption = np.array(caption)\n",
    "NLP_caption, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (1, 20))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "\n",
    "# declare a latent space\n",
    "latent_dim = 100\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "latent_points.shape, caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = g_model\n",
    "X = g_model.predict([latent_points, caption])#*400\n",
    "array = np.array(X.reshape(106,106),dtype = np.uint8)\n",
    "np.unique(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "array*=255\n",
    "new_image = Image.fromarray(array,'L')\n",
    "new_image = new_image.save(f'../data/midi_reconstruction/images/captioned_piece_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconvert MIDI images to MIDI files\n",
    "image_path = \"../data/midi_reconstruction/images/captioned_piece_test.png\"\n",
    "output_path = \"../data/midi_reconstruction\"\n",
    "\n",
    "img2midi_obj = img2midi(image_path, output_path, resolution=0.25)\n",
    "img2midi_obj.convert_to_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# run to cell to play\n",
    "# stop the cell and run sp.stop to stop the music\n",
    "from music21 import midi, converter, instrument, note, chord\n",
    "\n",
    "mf = midi.MidiFile()\n",
    "mf.open(f\"{output_path}/music.mid\")\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "sp = midi.realtime.StreamPlayer(s)\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museGAN_experiment_w_caption_v2.ipynb Cell 34\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sp\u001b[39m.\u001b[39mstop\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "sp.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
