{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mido in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: packaging~=23.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from mido) (23.1)\n",
      "Requirement already satisfied: pygame in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: music21 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (7.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ktrin\\appdata\\roaming\\python\\python39\\site-packages (from music21) (1.19.5)\n",
      "Requirement already satisfied: jsonpickle in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (3.5.2)\n",
      "Requirement already satisfied: webcolors>=1.5 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.12)\n",
      "Requirement already satisfied: joblib in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (1.3.2)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (8.13.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from music21) (5.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (4.34.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from matplotlib->music21) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ktrin\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ktrin\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn==1.3.0) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ktrin\\miniconda3\\envs\\w266\\lib\\site-packages (from scikit-learn==1.3.0) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mido\n",
    "!pip install pygame\n",
    "!pip install music21\n",
    "!pip install scikit-learn==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some useful libraries\n",
    "import glob, nltk, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from music21 import midi\n",
    "from plugins.midi2img import midi2img\n",
    "from plugins.img2midi import img2midi\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load In museGAN dataset for visualization purposes\n",
    "It turned out that the people at museGAN is leveraging midi -> image conversion. The image consisted of bar of a multi track piano roll. From the below image, the horizontal represent time and the vericle represent the instrument used. In this dataset the instrument are layered from bottom to top as piano, strings, guitar, drums, bass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ktrin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the punkt tokenizer from nltk to tokenize the piece caption\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>caption_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commu00343</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of lucid skies traversing ethereal re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commu07280</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>an enchanting symphony of soaring dreams where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commu02735</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>uncharted melody a harmonious journey through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commu06179</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of shadows a melodys dance in the twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commu03042</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of shadows a haunting symphony where ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              image  \\\n",
       "0  commu00343  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "1  commu07280  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "2  commu02735  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "3  commu06179  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "4  commu03042  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "\n",
       "                                        caption_list  \n",
       "0  whispers of lucid skies traversing ethereal re...  \n",
       "1  an enchanting symphony of soaring dreams where...  \n",
       "2  uncharted melody a harmonious journey through ...  \n",
       "3  whispers of shadows a melodys dance in the twi...  \n",
       "4  whispers of shadows a haunting symphony where ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training data\n",
    "# read in the training data\n",
    "training_set = joblib.load(f'{DATA_PATH}/training_set_desc.joblib')\n",
    "training_set[0]\n",
    "\n",
    "# conver tuple to DF\n",
    "training_set = pd.DataFrame(training_set, columns=['id', 'image', 'caption_list'])\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of museGAN\n",
    "## External Data Source\n",
    "For whatever reason, if we wanted to perform GAN modeling, we can leverage conversion of MIDI data to that of the piano roll. Download the data from piano repo in README and start performing the things below. [Convert-MIDI-TO-NP-ARRAY](https://medium.com/analytics-vidhya/convert-midi-file-to-numpy-array-in-python-7d00531890c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 20 # 18 + start, end\n",
    "EMBED_DIM = 100 \n",
    "MAX_VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load in the metadata\n",
    "# # create a list of captions that concatenate the piece description and arousal\n",
    "# # lower case te caption list\n",
    "# midi_meta = pd.read_csv('../data/piano-labelled/labelled_piano_midi_metadata.csv')\n",
    "# midi_meta['caption_list'] = midi_meta['piece_description'].str.lower()+ \". \" + midi_meta['piece_arousal'].str.lower()\n",
    "# midi_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372c7eb0a343481e9661dbd046eed15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build a vocabulary using sklearn count vectorizer to create a vocab from the most frequent words\n",
    "input_captions = []\n",
    "max_caption_length = -1 \n",
    "\n",
    "for caption in tqdm(training_set['caption_list'].values):\n",
    "    tokenized_caption = nltk.word_tokenize(caption, language='english')\n",
    "\n",
    "    if len(tokenized_caption) > max_caption_length:\n",
    "        max_caption_length = len(tokenized_caption)\n",
    "\n",
    "    caption = (' '.join(tokenized_caption)).lower()\n",
    "    input_captions.append(caption)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=MAX_VOCAB_SIZE)\n",
    "vectorizer.fit(input_captions)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "MAX_VOCAB_SIZE = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn vocab into a dictionary of words and token id\n",
    "# replace some words with special tokens like start/end/unk\n",
    "# if the caption is too short, pad it with <pad> token\n",
    "id_vocab_dict = {}\n",
    "vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab):\n",
    "    id_vocab_dict[sid] = svocab\n",
    "    vocab_id_dict[svocab] = sid\n",
    "\n",
    "id_vocab_dict[MAX_VOCAB_SIZE] = \"<unk>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 1] = \"<start>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 2] = \"<end>\"\n",
    "id_vocab_dict[MAX_VOCAB_SIZE + 3] = \"<pad>\"\n",
    "\n",
    "vocab_id_dict[\"<unk>\"] = MAX_VOCAB_SIZE\n",
    "vocab_id_dict[\"<start>\"] = MAX_VOCAB_SIZE + 1\n",
    "vocab_id_dict[\"<end>\"] = MAX_VOCAB_SIZE + 2\n",
    "vocab_id_dict[\"<pad>\"] = MAX_VOCAB_SIZE + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization - take the input caption and tokenize it\n",
    "# declare a max sequence length \n",
    "def convert_text_to_data(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None):\n",
    "    \"\"\"\n",
    "        Function to convert text based data into tokenized data with proper padding\n",
    "    \"\"\"\n",
    "\n",
    "    processed_data = []\n",
    "    for text_num, text in enumerate(texts):\n",
    "        sentence_ids = []\n",
    "\n",
    "        # split the sentence into token\n",
    "        # use the vocab to turn the word token into number\n",
    "        for token in text.split():\n",
    "            if token in vocab_id_dict.keys():\n",
    "                sentence_ids.append(vocab_id_dict[token])\n",
    "            else:\n",
    "                sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "\n",
    "        vocab_size = len(vocab_id_dict.keys())\n",
    "\n",
    "        # for decoder cases:\n",
    "        # input sentence: <start>, [tokenize words from vocab], <end>, padded with <unk>\n",
    "        # ouput sentence has: [tokenize words from vocab], <end>, padded with <unk>\n",
    "        if type == 'input_target':\n",
    "            ids = ([vocab_size - 3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        elif type == 'output_target':\n",
    "            ids = (sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "        processed_data.append(ids)\n",
    "\n",
    "    return np.array(processed_data)\n",
    "\n",
    "\n",
    "train_target_input_data = convert_text_to_data(input_captions,\n",
    "                                                vocab_id_dict,\n",
    "                                                type='input_target',\n",
    "                                                max_length=MAX_SEQ_LENGTH)\n",
    "len(train_target_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>caption_list</th>\n",
       "      <th>tokenized_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commu00343</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of lucid skies traversing ethereal re...</td>\n",
       "      <td>[1115, 1092, 669, 560, 822, 986, 353, 743, 111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commu07280</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>an enchanting symphony of soaring dreams where...</td>\n",
       "      <td>[1115, 61, 320, 924, 669, 833, 272, 1083, 438,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commu02735</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>uncharted melody a harmonious journey through ...</td>\n",
       "      <td>[1115, 995, 592, 1114, 439, 511, 952, 319, 743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commu06179</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of shadows a melodys dance in the twi...</td>\n",
       "      <td>[1115, 1092, 669, 810, 1114, 593, 226, 484, 94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commu03042</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of shadows a haunting symphony where ...</td>\n",
       "      <td>[1115, 1092, 669, 810, 1114, 446, 924, 1083, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>commu05422</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>melodic whispers under a starlit sky embracing...</td>\n",
       "      <td>[1115, 590, 1092, 996, 1114, 874, 825, 307, 94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>commu03260</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers in shadows an enchanting melody awake...</td>\n",
       "      <td>[1115, 1092, 484, 810, 61, 320, 592, 82, 1035,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>commu04717</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>whispers of cinematic dreams unfold as a young...</td>\n",
       "      <td>[1115, 1092, 669, 189, 272, 1002, 68, 1114, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>commu02652</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>lost in shadows the enigmatic serenade paintin...</td>\n",
       "      <td>[1115, 556, 484, 810, 943, 327, 801, 694, 1059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>commu10376</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>enchanting echoes of a whimsical journey a lus...</td>\n",
       "      <td>[1115, 320, 287, 669, 1114, 1085, 511, 1114, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              image  \\\n",
       "0    commu00343  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "1    commu07280  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "2    commu02735  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "3    commu06179  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "4    commu03042  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "..          ...                                                ...   \n",
       "190  commu05422  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "191  commu03260  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "192  commu04717  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "193  commu02652  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "194  commu10376  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "\n",
       "                                          caption_list  \\\n",
       "0    whispers of lucid skies traversing ethereal re...   \n",
       "1    an enchanting symphony of soaring dreams where...   \n",
       "2    uncharted melody a harmonious journey through ...   \n",
       "3    whispers of shadows a melodys dance in the twi...   \n",
       "4    whispers of shadows a haunting symphony where ...   \n",
       "..                                                 ...   \n",
       "190  melodic whispers under a starlit sky embracing...   \n",
       "191  whispers in shadows an enchanting melody awake...   \n",
       "192  whispers of cinematic dreams unfold as a young...   \n",
       "193  lost in shadows the enigmatic serenade paintin...   \n",
       "194  enchanting echoes of a whimsical journey a lus...   \n",
       "\n",
       "                                    tokenized_captions  \n",
       "0    [1115, 1092, 669, 560, 822, 986, 353, 743, 111...  \n",
       "1    [1115, 61, 320, 924, 669, 833, 272, 1083, 438,...  \n",
       "2    [1115, 995, 592, 1114, 439, 511, 952, 319, 743...  \n",
       "3    [1115, 1092, 669, 810, 1114, 593, 226, 484, 94...  \n",
       "4    [1115, 1092, 669, 810, 1114, 446, 924, 1083, 5...  \n",
       "..                                                 ...  \n",
       "190  [1115, 590, 1092, 996, 1114, 874, 825, 307, 94...  \n",
       "191  [1115, 1092, 484, 810, 61, 320, 592, 82, 1035,...  \n",
       "192  [1115, 1092, 669, 189, 272, 1002, 68, 1114, 11...  \n",
       "193  [1115, 556, 484, 810, 943, 327, 801, 694, 1059...  \n",
       "194  [1115, 320, 287, 669, 1114, 1085, 511, 1114, 5...  \n",
       "\n",
       "[195 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# added the tokenized caption to the metadata\n",
    "training_set['tokenized_captions'] = train_target_input_data.tolist()\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 106, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['image'].values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image, tokenized pair\n",
    "\n",
    "# create image-caption pairs\n",
    "datasets = []\n",
    "for i, row in training_set.iterrows():\n",
    "    caption = np.array(row['tokenized_captions'])\n",
    "    images = np.array(row['image'])\n",
    "    try:\n",
    "        datasets.append((images, caption))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select a random row from the metadata to get the caption\n",
    "# row = midi_meta.sample(1, random_state=22)\n",
    "\n",
    "# # get a random image tokenize caption and actual caption\n",
    "# NLP_caption = row['caption_list'].values\n",
    "# caption = [np.array(a) for a in row['tokenized_captions'].values]\n",
    "# caption = np.array(caption)\n",
    "# NLP_caption, caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Definition\n",
    "\n",
    "GAN model consists of two part:\n",
    "1. Generator\n",
    "2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# check to see if tensorflow mount to GPU properly\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_generator(latent_dim=100, \n",
    "                               caption_dim=MAX_SEQ_LENGTH, \n",
    "                               vocab_size=len(vocab_id_dict.keys()), \n",
    "                               embed_dim=EMBED_DIM):\n",
    "    \"\"\"Define the generator model\n",
    "        Inputs:\n",
    "            latent_dim: dimension of the latent space\n",
    "        Output:\n",
    "            model: the generator model\n",
    "    \"\"\"\n",
    "    n_nodes = 128 * 53 * 53\n",
    "\n",
    "    # vectorized input layers\n",
    "    input_layer = keras.layers.Input(shape=(latent_dim,), name='input_layer')\n",
    "    \n",
    "    # # vectorized caption input layers\n",
    "    # # apply word embedding to the caption\n",
    "    caption_input_layer = keras.layers.Input(shape=(caption_dim,), name='caption_input_layer')\n",
    "    embedding_layer  = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                output_dim=embed_dim,\n",
    "                                                name='caption_embedding_layer')\n",
    "    embed_caption = embedding_layer(caption_input_layer)\n",
    "\n",
    "    # # source_image_encoding = keras.layers.GlobalAveragePooling2D()(dense4)\n",
    "    # # using LSTM to encode the caption with the input layer\n",
    "    lstm_layer = keras.layers.LSTM(100, return_sequences=True, return_state=True, name=\"decoder_lstm_layer\")\n",
    "    decoder_output, decoder_state_h_output, decoder_state_c_output = lstm_layer(embed_caption, initial_state=[input_layer, input_layer])\n",
    "\n",
    "    # apply 1D Global Average Pooling to the output of the dense layer on the caption decoded\n",
    "    # global_average_pooling1d_layer = keras.layers.GlobalAveragePooling1D()(decoder_output)\n",
    "\n",
    "    # Dense Layer 1\n",
    "    dense1 = keras.layers.Dense(n_nodes)(decoder_state_c_output)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.35)(dense1)\n",
    "    reshape_layer = keras.layers.Reshape((53, 53, 128))(leaky_relu1)\n",
    "\n",
    "    # Dense Layer 2\n",
    "    dense2 =  keras.layers.Dense(1024)(reshape_layer)\n",
    "\n",
    "    # Conv2DTranspose Layer\n",
    "    conv2d_transpose = keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same')(dense2)\n",
    "\n",
    "    # Dense Layer 3\n",
    "    dense3 =  keras.layers.Dense(1024)(conv2d_transpose)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.35)(dense3)\n",
    "\n",
    "    # Dense Layer 4\n",
    "    dense4 =  keras.layers.Dense(1024)(leaky_relu2)\n",
    "\n",
    "    # Conv2D Layer\n",
    "    conv2d = keras.layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')(dense4)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[input_layer,caption_input_layer], outputs=conv2d, name='generator_model')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_discriminator(in_shape = (106,106,1)):\n",
    "    \"\"\"\n",
    "        GAN discriminator model\n",
    "        Inputs:\n",
    "            in_shape: shape of the input image\n",
    "        Output:\n",
    "            model: discriminator model with binary crossentropy loss to denotes if the image is real or fake\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = keras.layers.Input(shape=in_shape, name='input_layer')\n",
    "    \n",
    "    # 2D Convlution Layer 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(input_layer)\n",
    "    leaky_relu1 = keras.layers.LeakyReLU(alpha=0.2)(conv1)\n",
    "    dropout1 = keras.layers.Dropout(0.5)(leaky_relu1)\n",
    " \n",
    "    # 2D Convlution Layer 2\n",
    "    conv2 = keras.layers.Conv2D(64, (3,3), strides=(2, 2), padding='same')(dropout1)\n",
    "    leaky_relu2 = keras.layers.LeakyReLU(alpha=0.2)(conv2)\n",
    "    dropout2 = keras.layers.Dropout(0.5)(leaky_relu2)\n",
    "\n",
    "    # Flatten Layer\n",
    "    flatten_layer = keras.layers.Flatten()(dropout2)\n",
    "\n",
    "    # Batch Normalization Layer\n",
    "    batch_normalization = keras.layers.BatchNormalization()(flatten_layer)\n",
    "\n",
    "    # Dense Output Disminator Layer\n",
    "    discriminate_layer = keras.layers.Dense(1, activation='sigmoid')(batch_normalization)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=input_layer, outputs=discriminate_layer, name='discriminator_model')\n",
    "    \n",
    "    # model compile\n",
    "    # opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_enhanced_miniGAN(g_model, d_model,\n",
    "                             g_model_input_shape=100, \n",
    "                             g_model_caption_input_shape=MAX_SEQ_LENGTH):\n",
    "    \"\"\"\n",
    "        GAN model architecture\n",
    "        Inputs:\n",
    "            g_model: generator model\n",
    "            d_model: discriminator model\n",
    "            g_model_input_shape: shape of the input to the generator model\n",
    "            g_model_caption_input_shape: shape of the input caption to the generator model\n",
    "        Output:\n",
    "            model: GAN model\n",
    "    \"\"\"\n",
    "    # Pause the training of the discriminator\n",
    "    d_model.trainable = False\n",
    "\n",
    "    # Define the input layer for the generator\n",
    "    generator_input = keras.layers.Input(shape=(g_model_input_shape))  # Specify the shape of the generator's input\n",
    "    caption_input = keras.layers.Input(shape=(g_model_caption_input_shape))  # Specify the shape of the generator's input\n",
    "\n",
    "    # Define the output of the generator\n",
    "    generator_output = g_model([generator_input, caption_input])\n",
    "\n",
    "    # Define the output of the discriminator\n",
    "    discriminator_output = d_model(generator_output)\n",
    "\n",
    "    # Create the model\n",
    "    model = keras.Model(inputs=[generator_input, caption_input], outputs=discriminator_output)\n",
    "\n",
    "    # # Compile the model\n",
    "    # opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, beta_2=0.9, clipnorm=1.0)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.d_gradient = []\n",
    "        self.g_gradient = []\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        # Unpack the data\n",
    "        real_images, real_captions = dataset\n",
    "\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "\n",
    "            # Get the latent vector (random noise)\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                \n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_loss = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_gradient.append((i, d_gradient))\n",
    "\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator([random_latent_vectors, real_captions], training=True)\n",
    "\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        self.g_gradient.append(gen_gradient)\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # slice the image and caption from the dataset\n",
    "    # generate 'real' class labels (1)\n",
    "    images, captions = zip(*dataset)\n",
    "    images = np.array(images)\n",
    "    captions = np.array(captions)\n",
    "    ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    X_img = images[ix]   \n",
    "    X_cap = captions[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X_img, X_cap, y\n",
    " \n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(g_model, latent_dim, caption, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = g_model.predict([x_input, caption])\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input_layer (InputLayer [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "caption_embedding_layer (Embedd (None, 10, 100)      6000        caption_input_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_layer (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_layer (LSTM)       [(None, 10, 100), (N 80400       caption_embedding_layer[0][0]    \n",
      "                                                                 input_layer[0][0]                \n",
      "                                                                 input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 359552)       36314752    decoder_lstm_layer[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 359552)       0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 53, 53, 128)  0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 53, 53, 1024) 132096      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 106, 106, 102 16778240    dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 106, 106, 102 1049600     conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 106, 106, 102 0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 106, 106, 102 1049600     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 106, 106, 1)  50177       dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 55,460,865\n",
      "Trainable params: 55,460,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"discriminator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 106, 106, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 53, 53, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 46656)             186624    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 46657     \n",
      "=================================================================\n",
      "Total params: 270,849\n",
      "Trainable params: 177,537\n",
      "Non-trainable params: 93,312\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "g_model = caption_enhanced_generator(latent_dim)\n",
    "d_model = caption_enhanced_discriminator()\n",
    "g_model.summary(), d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(g_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(d_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the generated model\n",
    "# keras.utils.plot_model(gan_model, show_shapes=True, dpi=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "import random\n",
    "random.seed(5634)\n",
    "\n",
    "# training_set = joblib.load('../data/piano-labelled/training_set.joblib')\n",
    "training_dataset = random.sample(datasets, 4096) #5 is the lenth of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (4096, 106, 106, 1))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unzip the dataset\n",
    "images, captions = zip(*training_dataset)\n",
    "images = np.array(images)\n",
    "captions = np.array(captions)\n",
    "type(images), images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ktrin\\miniconda3\\envs\\W266\\lib\\site-packages\\keras\\backend.py:4993: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 1700s 3s/step - d_loss: 1.3967 - g_loss: 0.6958\n",
      "Epoch 2/3\n",
      "512/512 [==============================] - 1654s 3s/step - d_loss: 1.3875 - g_loss: 0.6931\n",
      "Epoch 3/3\n",
      "512/512 [==============================] - 1653s 3s/step - d_loss: 1.3866 - g_loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e922910220>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set some training parameters\n",
    "BATCH_SIZE = 8\n",
    "noise_dim = 100\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    # real_loss = tf.reduce_mean(real_img)\n",
    "    # fake_loss = tf.reduce_mean(fake_img)\n",
    "\n",
    "    real_loss = cross_entropy(tf.ones_like(real_img), real_img)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_img), fake_img) \n",
    "    return fake_loss + real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    # return -tf.reduce_mean(fake_img)\n",
    "    return cross_entropy(tf.ones_like(fake_img), fake_img)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "# cbk = GANMonitor(num_img=3, latent_dim=noise_dim, filepath='../model/wgan-callbacks')\n",
    "\n",
    "# Get the wgan model\n",
    "gan = GAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "gan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "gan.fit(images, captions, batch_size=BATCH_SIZE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "g_model.save('../models/mini-gan/comMU_generator_model_g3.h5')\n",
    "d_model.save('../models/mini-gan/comMU_discriminator_model_g3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "g_model = keras.models.load_model('../models/mini-gan/comMU_generator_model_g3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_meta = pd.read_csv(f'{DATA_PATH}/commu_meta_caption.csv')\n",
    "# midi_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>caption_list</th>\n",
       "      <th>tokenized_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>commu00001</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>a, minor, mid, main, melody, cinematic, string...</td>\n",
       "      <td>[57, 56, 56, 34, 56, 33, 56, 29, 56, 32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>commu00002</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>c, major, mid, low, accompaniment, newage, aco...</td>\n",
       "      <td>[57, 56, 56, 30, 56, 33, 56, 28, 56, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commu00003</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>a, minor, mid, high, riff, cinematic, string, ...</td>\n",
       "      <td>[57, 56, 56, 34, 56, 33, 56, 26, 56, 42]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>commu00004</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>c, major, mid, pad, cinematic, choir</td>\n",
       "      <td>[57, 56, 56, 30, 56, 33, 56, 39, 56, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>commu00005</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>a, minor, mid, low, pad, cinematic, acoustic, ...</td>\n",
       "      <td>[57, 56, 56, 34, 56, 33, 56, 28, 56, 39]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241</th>\n",
       "      <td>commu10377</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>c, major, low, bass, cinematic, electric, bass</td>\n",
       "      <td>[57, 56, 56, 30, 56, 28, 56, 8, 56, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10242</th>\n",
       "      <td>commu10378</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>a, minor, mid, pad, cinematic, string, ensembl...</td>\n",
       "      <td>[57, 56, 56, 34, 56, 33, 56, 39, 56, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>commu10379</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>a, minor, mid, pad, cinematic, string, ensemble</td>\n",
       "      <td>[57, 56, 56, 34, 56, 33, 56, 39, 56, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>commu10380</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>c, major, low, bass, cinematic, acoustic, piano</td>\n",
       "      <td>[57, 56, 56, 30, 56, 28, 56, 8, 56, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>commu10381</td>\n",
       "      <td>[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...</td>\n",
       "      <td>a, minor, mid, high, sub, melody, cinematic, s...</td>\n",
       "      <td>[57, 56, 56, 34, 56, 33, 56, 26, 56, 44]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10246 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                              image  \\\n",
       "0      commu00001  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "1      commu00002  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "2      commu00003  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "3      commu00004  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "4      commu00005  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "...           ...                                                ...   \n",
       "10241  commu10377  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "10242  commu10378  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "10243  commu10379  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "10244  commu10380  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "10245  commu10381  [[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0...   \n",
       "\n",
       "                                            caption_list  \\\n",
       "0      a, minor, mid, main, melody, cinematic, string...   \n",
       "1      c, major, mid, low, accompaniment, newage, aco...   \n",
       "2      a, minor, mid, high, riff, cinematic, string, ...   \n",
       "3                   c, major, mid, pad, cinematic, choir   \n",
       "4      a, minor, mid, low, pad, cinematic, acoustic, ...   \n",
       "...                                                  ...   \n",
       "10241     c, major, low, bass, cinematic, electric, bass   \n",
       "10242  a, minor, mid, pad, cinematic, string, ensembl...   \n",
       "10243    a, minor, mid, pad, cinematic, string, ensemble   \n",
       "10244    c, major, low, bass, cinematic, acoustic, piano   \n",
       "10245  a, minor, mid, high, sub, melody, cinematic, s...   \n",
       "\n",
       "                             tokenized_captions  \n",
       "0      [57, 56, 56, 34, 56, 33, 56, 29, 56, 32]  \n",
       "1       [57, 56, 56, 30, 56, 33, 56, 28, 56, 5]  \n",
       "2      [57, 56, 56, 34, 56, 33, 56, 26, 56, 42]  \n",
       "3      [57, 56, 56, 30, 56, 33, 56, 39, 56, 15]  \n",
       "4      [57, 56, 56, 34, 56, 33, 56, 28, 56, 39]  \n",
       "...                                         ...  \n",
       "10241   [57, 56, 56, 30, 56, 28, 56, 8, 56, 15]  \n",
       "10242  [57, 56, 56, 34, 56, 33, 56, 39, 56, 15]  \n",
       "10243  [57, 56, 56, 34, 56, 33, 56, 39, 56, 15]  \n",
       "10244   [57, 56, 56, 30, 56, 28, 56, 8, 56, 15]  \n",
       "10245  [57, 56, 56, 34, 56, 33, 56, 26, 56, 44]  \n",
       "\n",
       "[10246 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['c, major, mid, high, main, melody, newage, acoustic, piano'],\n",
       "       dtype=object),\n",
       " array([[57, 56, 56, 30, 56, 33, 56, 26, 56, 29]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a random row from the metadata to get the caption\n",
    "row = training_set.sample(1, random_state=756)\n",
    "\n",
    "# get a random image tokenize caption and actual caption\n",
    "NLP_caption = row['caption_list'].values\n",
    "caption = [np.array(a) for a in row['tokenized_captions'].values]\n",
    "caption = np.array(caption)\n",
    "NLP_caption, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 100), (1, 10))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "\n",
    "# declare a latent space\n",
    "latent_dim = 100\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "latent_points.shape, caption.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = g_model\n",
    "X = g_model.predict([latent_points, caption])#*400\n",
    "array = np.array(X.reshape(106,106),dtype = np.uint8)\n",
    "np.unique(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "array*=255\n",
    "new_image = Image.fromarray(array,'L')\n",
    "new_image = new_image.save(f'../data/midi_reconstruction/images/captioned_piece_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconvert MIDI images to MIDI files\n",
    "image_path = \"../data/midi_reconstruction/images/captioned_piece_test.png\"\n",
    "output_path = \"../data/midi_reconstruction\"\n",
    "\n",
    "img2midi_obj = img2midi(image_path, output_path, resolution=0.25)\n",
    "img2midi_obj.convert_to_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# run to cell to play\n",
    "# stop the cell and run sp.stop to stop the music\n",
    "from music21 import midi, converter, instrument, note, chord\n",
    "\n",
    "mf = midi.MidiFile()\n",
    "mf.open(f\"{output_path}/music.mid\")\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi.translate.midiFileToStream(mf)\n",
    "sp = midi.realtime.StreamPlayer(s)\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\UNIXSpace\\Git_Desktop\\Berkeley-MIDS\\Classes\\W210 - Capstone\\w210-capstone-fall2023\\workspace\\museGAN_experiment_w_caption_v2.ipynb Cell 34\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/UNIXSpace/Git_Desktop/Berkeley-MIDS/Classes/W210%20-%20Capstone/w210-capstone-fall2023/workspace/museGAN_experiment_w_caption_v2.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sp\u001b[39m.\u001b[39mstop\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "sp.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.d_gradient = []\n",
    "        self.g_gradient = []\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        \"\"\"\n",
    "            TODO: incorporate trainign metric\n",
    "        \"\"\"\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        self.g_metric = [tf.keras.metrics.AUC(), tf.keras.metrics.Accuracy()]\n",
    "\n",
    "\n",
    "    def train_step(self, dataset):\n",
    "        # Unpack the data\n",
    "        characteristics_curves, econ_data, target_ranking = dataset\n",
    "\n",
    "        if isinstance(characteristics_curves, tuple):\n",
    "            characteristics_curves = characteristics_curves[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(characteristics_curves)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # have the generator generate the risk probability and shortage ranking\n",
    "                fake_risk_logits, fake_ranking = self.generator([characteristics_curves, econ_data], training=True)\n",
    "\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_ranking, training=True)\n",
    "                \n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(target_ranking, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_loss = self.d_loss_fn(real=real_logits, fake=fake_logits)\n",
    "\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_gradient.append((i, d_gradient))\n",
    "\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_ranking, generated_ranking = self.generator([characteristics_curves, econ_data], training=True)\n",
    "\n",
    "\n",
    "            # apply accuracy metric calculation on generated_ranking\n",
    "            accuracy = tf.keras.metrics.Accuracy()\n",
    "            accuracy.update_state(target_ranking, generated_ranking)\n",
    "\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_ranking, training=True)\n",
    "\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        self.g_gradient.append(gen_gradient)\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
